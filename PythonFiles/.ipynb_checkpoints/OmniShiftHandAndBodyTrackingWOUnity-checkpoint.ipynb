{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library version: 2.4.0.144\n",
      "Number of cameras detected: 1\n",
      "Running example for camera 0...\n",
      "*** IMAGE ACQUISITION ***\n",
      "\n",
      "Acquisition mode set to continuous...\n",
      "Acquiring images...\n",
      "Device serial number retrieved as 20304031...\n",
      "Press enter to close the program..\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-453d453d693b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    977\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-453d453d693b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Running example for camera %d...'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 955\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[0mrun_single_camera\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    956\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Camera %d example complete... \\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-453d453d693b>\u001b[0m in \u001b[0;36mrun_single_camera\u001b[1;34m(cam)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;31m# Acquire images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[0macquire_and_display_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodemap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodemap_tldevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m         \u001b[1;31m# Deinitialize camera\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-453d453d693b>\u001b[0m in \u001b[0;36macquire_and_display_images\u001b[1;34m(cam, nodemap, nodemap_tldevice)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                             \u001b[1;31m## **** Mediapipe Pose Detection ****\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                             \u001b[0mposeResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpose\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_rgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \"\"\"\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mlandmark\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpose_landmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlandmark\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mediapipe\\python\\solution_base.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    320\u001b[0m       elif (input_stream_type == _PacketDataType.IMAGE_FRAME or\n\u001b[0;32m    321\u001b[0m             input_stream_type == _PacketDataType.IMAGE):\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mRGB_CHANNELS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input image must contain three channel rgb data.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         self._graph.add_packet_to_input_stream(\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Fast cam acquisition and MediaPipe POSE recognition and gesture recognition\n",
    "\n",
    "\n",
    "# coding=utf-8\n",
    "# =============================================================================\n",
    "# Copyright (c) 2001-2021 FLIR Systems, Inc. All Rights Reserved.\n",
    "#\n",
    "# This software is the confidential and proprietary information of FLIR\n",
    "# Integrated Imaging Solutions, Inc. (\"Confidential Information\"). You\n",
    "# shall not disclose such Confidential Information and shall use it only in\n",
    "# accordance with the terms of the license agreement you entered into\n",
    "# with FLIR Integrated Imaging Solutions, Inc. (FLIR).\n",
    "#\n",
    "# FLIR MAKES NO REPRESENTATIONS OR WARRANTIES ABOUT THE SUITABILITY OF THE\n",
    "# SOFTWARE, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR\n",
    "# PURPOSE, OR NON-INFRINGEMENT. FLIR SHALL NOT BE LIABLE FOR ANY DAMAGES\n",
    "# SUFFERED BY LICENSEE AS A RESULT OF USING, MODIFYING OR DISTRIBUTING\n",
    "# THIS SOFTWARE OR ITS DERIVATIVES.\n",
    "# =============================================================================\n",
    "#\n",
    "# This AcquireAndDisplay.py shows how to get the image data, and then display images in a GUI.\n",
    "# This example relies on information provided in the ImageChannelStatistics.py example.\n",
    "#\n",
    "# This example demonstrates how to display images represented as numpy arrays.\n",
    "# Currently, this program is limited to single camera use.\n",
    "# NOTE: keyboard and matplotlib must be installed on Python interpreter prior to running this example.\n",
    "\n",
    "import os\n",
    "from pyspin import PySpin\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import keyboard\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Mediapipe hand tracking and OpenCV and Pillow\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "\n",
    "## gesture recognition\n",
    "from utils import CvFpsCalc\n",
    "from model import KeyPointClassifier\n",
    "from model import PointHistoryClassifier\n",
    "\n",
    "import csv\n",
    "import copy\n",
    "import argparse\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "global continue_recording\n",
    "continue_recording = True\n",
    "\n",
    "BG_COLOR = (0, 0, 0) # black\n",
    "\n",
    "\n",
    "def handle_close(evt):\n",
    "    \"\"\"\n",
    "    This function will close the GUI when close event happens.\n",
    "\n",
    "    :param evt: Event that occurs when the figure closes.\n",
    "    :type evt: Event\n",
    "    \"\"\"\n",
    "\n",
    "    global continue_recording\n",
    "    continue_recording = False\n",
    "\n",
    "\n",
    "def acquire_and_display_images(cam, nodemap, nodemap_tldevice):\n",
    "    \"\"\"\n",
    "    This function continuously acquires images from a device and display them in a GUI.\n",
    "\n",
    "    :param cam: Camera to acquire images from.\n",
    "    :param nodemap: Device nodemap.\n",
    "    :param nodemap_tldevice: Transport layer device nodemap.\n",
    "    :type cam: CameraPtr\n",
    "    :type nodemap: INodeMap\n",
    "    :type nodemap_tldevice: INodeMap\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    global continue_recording\n",
    "\n",
    "    sNodemap = cam.GetTLStreamNodeMap()\n",
    "\n",
    "    # Change bufferhandling mode to NewestOnly\n",
    "    node_bufferhandling_mode = PySpin.CEnumerationPtr(sNodemap.GetNode('StreamBufferHandlingMode'))\n",
    "    if not PySpin.IsAvailable(node_bufferhandling_mode) or not PySpin.IsWritable(node_bufferhandling_mode):\n",
    "        print('Unable to set stream buffer handling mode.. Aborting...')\n",
    "        return False\n",
    "\n",
    "    # Retrieve entry node from enumeration node\n",
    "    node_newestonly = node_bufferhandling_mode.GetEntryByName('NewestOnly')\n",
    "    if not PySpin.IsAvailable(node_newestonly) or not PySpin.IsReadable(node_newestonly):\n",
    "        print('Unable to set stream buffer handling mode.. Aborting...')\n",
    "        return False\n",
    "\n",
    "    # Retrieve integer value from entry node\n",
    "    node_newestonly_mode = node_newestonly.GetValue()\n",
    "\n",
    "    # Set integer value from entry node as new value of enumeration node\n",
    "    node_bufferhandling_mode.SetIntValue(node_newestonly_mode)\n",
    "\n",
    "    print('*** IMAGE ACQUISITION ***\\n')\n",
    "    try:\n",
    "        ## gesture recognition\n",
    "        result = True\n",
    "        use_brect = True\n",
    "        \n",
    "        ## media pipe\n",
    "        node_acquisition_mode = PySpin.CEnumerationPtr(nodemap.GetNode('AcquisitionMode'))\n",
    "        if not PySpin.IsAvailable(node_acquisition_mode) or not PySpin.IsWritable(node_acquisition_mode):\n",
    "            print('Unable to set acquisition mode to continuous (enum retrieval). Aborting...')\n",
    "            return False\n",
    "\n",
    "        # Retrieve entry node from enumeration node\n",
    "        node_acquisition_mode_continuous = node_acquisition_mode.GetEntryByName('Continuous')\n",
    "        if not PySpin.IsAvailable(node_acquisition_mode_continuous) or not PySpin.IsReadable(\n",
    "                node_acquisition_mode_continuous):\n",
    "            print('Unable to set acquisition mode to continuous (entry retrieval). Aborting...')\n",
    "            return False\n",
    "\n",
    "        # Retrieve integer value from entry node\n",
    "        acquisition_mode_continuous = node_acquisition_mode_continuous.GetValue()\n",
    "\n",
    "        # Set integer value from entry node as new value of enumeration node\n",
    "        node_acquisition_mode.SetIntValue(acquisition_mode_continuous)\n",
    "\n",
    "        print('Acquisition mode set to continuous...')\n",
    "\n",
    "        #  Begin acquiring images\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  What happens when the camera begins acquiring images depends on the\n",
    "        #  acquisition mode. Single frame captures only a single image, multi\n",
    "        #  frame catures a set number of images, and continuous captures a\n",
    "        #  continuous stream of images.\n",
    "        #\n",
    "        #  *** LATER ***\n",
    "        #  Image acquisition must be ended when no more images are needed.\n",
    "        cam.BeginAcquisition()\n",
    "\n",
    "        print('Acquiring images...')\n",
    "\n",
    "        #  Retrieve device serial number for filename\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  The device serial number is retrieved in order to keep cameras from\n",
    "        #  overwriting one another. Grabbing image IDs could also accomplish\n",
    "        #  this.\n",
    "        device_serial_number = ''\n",
    "        node_device_serial_number = PySpin.CStringPtr(nodemap_tldevice.GetNode('DeviceSerialNumber'))\n",
    "        if PySpin.IsAvailable(node_device_serial_number) and PySpin.IsReadable(node_device_serial_number):\n",
    "            device_serial_number = node_device_serial_number.GetValue()\n",
    "            print('Device serial number retrieved as %s...' % device_serial_number)\n",
    "\n",
    "        # Close program\n",
    "        print('Press enter to close the program..')\n",
    "\n",
    "        # Figure(1) is default so you can omit this line. Figure(0) will create a new window every time program hits this line\n",
    "        fig = plt.figure(1)\n",
    "\n",
    "        # Close the GUI when close event happens\n",
    "        fig.canvas.mpl_connect('close_event', handle_close)\n",
    "\n",
    "        \n",
    "        ## this is for the pose aquisition\n",
    "        with mp_pose.Pose(\n",
    "            #model_complexity=2,\n",
    "            #static_image_mode=True,\n",
    "            enable_segmentation=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as pose:\n",
    "        \n",
    "        \n",
    "            ## this is the hand detection loop (maybe I can later feed the cropped hands from the pose loop only?)\n",
    "            # Retrieve and display images\n",
    "            with mp_hands.Hands(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "                ## for gesture recognition\n",
    "                keypoint_classifier = KeyPointClassifier()\n",
    "\n",
    "                point_history_classifier = PointHistoryClassifier()\n",
    "\n",
    "                # Read labels ###########################################################\n",
    "                with open('model/keypoint_classifier/keypoint_classifier_label.csv', encoding='utf-8-sig') as f:\n",
    "                    keypoint_classifier_labels = csv.reader(f)\n",
    "                    keypoint_classifier_labels = [row[0] for row in keypoint_classifier_labels]\n",
    "                with open('model/point_history_classifier/point_history_classifier_label.csv', encoding='utf-8-sig') as f:\n",
    "                    point_history_classifier_labels = csv.reader(f)\n",
    "                    point_history_classifier_labels = [row[0] for row in point_history_classifier_labels]\n",
    "\n",
    "                # FPS Measurement ########################################################\n",
    "                cvFpsCalc = CvFpsCalc(buffer_len=10)\n",
    "\n",
    "                # Coordinate history #################################################################\n",
    "                history_length = 16\n",
    "                point_history = deque(maxlen=history_length)\n",
    "\n",
    "                # Finger gesture history ################################################\n",
    "                finger_gesture_history = deque(maxlen=history_length)\n",
    "\n",
    "                #  ########################################################################\n",
    "\n",
    "                mode = 0\n",
    "\n",
    "                while(continue_recording):\n",
    "                    try:\n",
    "\n",
    "\n",
    "                        fps = cvFpsCalc.get()\n",
    "                        # Process Key (ESC: end) #################################################\n",
    "                        key = cv2.waitKey(1)\n",
    "                        if key == 27:  # ESC\n",
    "                            break\n",
    "                        number, mode = select_mode(key, mode)\n",
    "\n",
    "                        #  Retrieve next received image\n",
    "                        #\n",
    "                        #  *** NOTES ***\n",
    "                        #  Capturing an image houses images on the camera buffer. Trying\n",
    "                        #  to capture an image that does not exist will hang the camera.\n",
    "                        #\n",
    "                        #  *** LATER ***\n",
    "                        #  Once an image from the buffer is saved and/or no longer\n",
    "                        #  needed, the image must be released in order to keep the\n",
    "                        #  buffer from filling up.\n",
    "\n",
    "                        image_result = cam.GetNextImage(1000)\n",
    "\n",
    "                        #  Ensure image completion\n",
    "                        if image_result.IsIncomplete():\n",
    "                            print('Image incomplete with image status %d ...' % image_result.GetImageStatus())\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            ## This is 2048x2048 so we need to crop it to the ROI from the lense\n",
    "                            width = image_result.GetWidth()\n",
    "                            height = image_result.GetHeight()\n",
    "\n",
    "\n",
    "                            ## This converts it to GreyScale\n",
    "                            #image_converted = image_result.Convert(spin.PixelFormat_Mono8, spin.HQ_LINEAR)\n",
    "                            ## This converts it to RGB\n",
    "                            image_converted = image_result.Convert(PySpin.PixelFormat_BGR8)\n",
    "                            rgb_array = image_converted.GetData()\n",
    "                            rgb_array = rgb_array.reshape(height, width, 3)\n",
    "\n",
    "\n",
    "                            ## process mediapipe on image\n",
    "                            #image_rgb = cv2.cvtColor(cv2.flip(rgb_array, 1), cv2.COLOR_BGR2RGB)\n",
    "                            image_rgb = cv2.flip(rgb_array, 1)\n",
    "\n",
    "\n",
    "                            ## **** Resizing / Croping *****\n",
    "\n",
    "                            ## RESIZING the image since it would be 2048x2048 otherwise (kind of too big for the window)\n",
    "\n",
    "                            ## we might have to size this further down for mediapipe to run fast\n",
    "                            #image_rgb = cv2.resize(image_rgb, (800, 800))\n",
    "                            # scale = 800/2048   \n",
    "                            ## this is to display potential cropping region in the downscaled image\n",
    "                            # cv2.rectangle(image_rgb, (int(650*scale), int(580*scale)), (int(1450*scale), int(1380*scale)), (0, 255, 0), 3)\n",
    "\n",
    "                            ## CROPPPING the region of the lense (should be around 800 to fit with the setup so far...)\n",
    "                            ## This is the cropping\n",
    "                            ## These values are taken from the unity config\n",
    "                            ## They might change...\n",
    "                            #array_cropped = image_rgb[ROI_y:(ROI_y + ROI_height), ROI_x:(ROI_x + ROI_width)]\n",
    "                            #image_rgb = array_cropped.copy() # needed to get the correct data format for further processing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            ## **** un-distort image\n",
    "\n",
    "                            #undistorted_img = cv2.remap(image_rgb, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "                            ## media pipe works better on rgb image\n",
    "                            #image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB)\n",
    "                            image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2GRAY)\n",
    "                            image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_GRAY2BGR)\n",
    "                            image_rgb = cv2.resize(image_rgb, (800, 800))\n",
    "\n",
    "\n",
    "                            ## **** Mediapipe Pose Detection ****\n",
    "                            poseResults = pose.process(image_rgb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            ## **** Mediapipe hand detection ****\n",
    "                            image_rgb.flags.writeable = False\n",
    "                            results = hands.process(image_rgb)\n",
    "                            image_rgb.flags.writeable = True\n",
    "\n",
    "                            ## convert image back to bgr for outputting it in rgb with the cam (python is confusing...)\n",
    "                            image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                            # Draw the hand annotations on the image.\n",
    "                            #if results.multi_hand_landmarks:\n",
    "                            #    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                            #        mp_drawing.draw_landmarks(\n",
    "                            #            image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            #gesture recongition output and pose recognition output\n",
    "                            debug_image = copy.deepcopy(image_rgb)\n",
    "                            pose_image = copy.deepcopy(image_rgb)\n",
    "\n",
    "\n",
    "                            output_image = np.zeros(pose_image.shape, dtype=np.uint8)\n",
    "                            output_image[:] = BG_COLOR\n",
    "                            if poseResults.pose_landmarks:\n",
    "                                ## Draw the pose annotations on the image\n",
    "#                                 mp_drawing.draw_landmarks(\n",
    "#                                     pose_image,\n",
    "#                                     poseResults.pose_landmarks,\n",
    "#                                     mp_pose.POSE_CONNECTIONS,\n",
    "#                                     landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()) \n",
    "\n",
    "                                #annotated_image = pose_image.copy()\n",
    "                                # Draw segmentation on the image.\n",
    "                                # To improve segmentation around boundaries, consider applying a joint\n",
    "                                # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "                                 condition = np.stack((poseResults.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "                                 bg_image = np.zeros(pose_image.shape, dtype=np.uint8)\n",
    "                                 bg_image[:] = BG_COLOR\n",
    "                                 output_image = np.where(condition, pose_image, bg_image)\n",
    "                                \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            # Draw the hand annotations on the image.\n",
    "                            if results.multi_hand_landmarks:\n",
    "                                for hand_landmarks, handedness in zip(results.multi_hand_landmarks,results.multi_handedness):\n",
    "                                    ## draw hand annotations into rgb\n",
    "                                    mp_drawing.draw_landmarks(image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                                    \n",
    "                                    ## draw hand annotations into pose\n",
    "                                    mp_drawing.draw_landmarks(pose_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                                    ## gesture recognition\n",
    "\n",
    "                                    # Bounding box calculation\n",
    "                                    brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
    "                                    # Landmark calculation\n",
    "                                    landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "                                    ## Landmark calcualtion with 3 coordinates\n",
    "                                    hand_landmarks_list = get_hand_landmarks_list(debug_image, hand_landmarks)\n",
    "\n",
    "                                    # Conversion to relative coordinates / normalized coordinates\n",
    "                                    pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "                                    pre_processed_point_history_list = pre_process_point_history(debug_image, point_history)\n",
    "                                    # Write to the dataset file\n",
    "                                    logging_csv(number, mode, pre_processed_landmark_list, pre_processed_point_history_list)\n",
    "\n",
    "                                    # Hand sign classification\n",
    "                                    hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "                                    if hand_sign_id == 2:  # Point gesture\n",
    "                                        point_history.append(landmark_list[8])\n",
    "                                    else:\n",
    "                                        point_history.append([0, 0])\n",
    "\n",
    "                                    # Finger gesture classification\n",
    "                                    finger_gesture_id = 0\n",
    "                                    point_history_len = len(pre_processed_point_history_list)\n",
    "                                    if point_history_len == (history_length * 2):\n",
    "                                        finger_gesture_id = point_history_classifier(pre_processed_point_history_list)\n",
    "\n",
    "                                    # Calculates the gesture IDs in the latest detection\n",
    "                                    finger_gesture_history.append(finger_gesture_id)\n",
    "                                    most_common_fg_id = Counter(finger_gesture_history).most_common()\n",
    "\n",
    "                                    # Drawing part\n",
    "                                    debug_image = draw_bounding_rect(use_brect, debug_image, brect)\n",
    "                                    debug_image = draw_landmarks(debug_image, landmark_list)\n",
    "                                    debug_image = draw_info_text(\n",
    "                                        debug_image,\n",
    "                                        brect,\n",
    "                                        handedness,\n",
    "                                        keypoint_classifier_labels[hand_sign_id],\n",
    "                                        point_history_classifier_labels[most_common_fg_id[0][0]],\n",
    "                                    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            cv2.imshow('MediaPipe Pose', pose_image)         \n",
    "                            #cv2.imshow('Hand Gesture Recognition', debug_image)    \n",
    "                            #cv2.imshow('MediaPipe Hands', image_rgb)\n",
    "                            cv2.imshow('MediaPipe Pose Segmentation mask', output_image)  \n",
    "                            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                                break\n",
    "                                \n",
    "                        #image_result.Release()\n",
    "\n",
    "                        # Getting the image data as a numpy array\n",
    "                        #image_data = image_result.GetNDArray()\n",
    "\n",
    "                        # Draws an image on the current figure\n",
    "                        #plt.imshow(image_data, cmap='gray')\n",
    "\n",
    "                        # Interval in plt.pause(interval) determines how fast the images are displayed in a GUI\n",
    "                        # Interval is in seconds.\n",
    "                        #plt.pause(0.001)\n",
    "\n",
    "                        # Clear current reference of a figure. This will improve display speed significantly\n",
    "                        #plt.clf()\n",
    "\n",
    "                        # If user presses enter, close the program\n",
    "                        if keyboard.is_pressed('ENTER'):\n",
    "                            print('Program is closing...')\n",
    "\n",
    "                            # Close figure\n",
    "                            plt.close('all')             \n",
    "                            input('Done! Press Enter to exit...')\n",
    "                            continue_recording=False                        \n",
    "\n",
    "                        #  Release image\n",
    "                        #\n",
    "                        #  *** NOTES ***\n",
    "                        #  Images retrieved directly from the camera (i.e. non-converted\n",
    "                        #  images) need to be released in order to keep from filling the\n",
    "                        #  buffer.\n",
    "                        image_result.Release()\n",
    "\n",
    "                    except PySpin.SpinnakerException as ex:\n",
    "                        print('Error: %s' % ex)\n",
    "                        return False\n",
    "\n",
    "        #  End acquisition\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  Ending acquisition appropriately helps ensure that devices clean up\n",
    "        #  properly and do not need to be power-cycled to maintain integrity.\n",
    "        cam.EndAcquisition()\n",
    "\n",
    "    except PySpin.SpinnakerException as ex:\n",
    "        print('Error: %s' % ex)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gesture recognition\n",
    "\n",
    "## converting hand_landmarks to sendable vector3 string list?\n",
    "## returns a list of all handmarks and their 3 coordinates (normalized)\n",
    "def get_hand_landmarks_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = landmark.x\n",
    "        landmark_y = landmark.y\n",
    "        landmark_z = landmark.z\n",
    "        #landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        #landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        #landmark_z = landmark.z\n",
    "\n",
    "        \n",
    "        ## need the brackets for this function\n",
    "        landmark_point.append([landmark_x, landmark_y, landmark_z])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gesture recognition functions\n",
    "\n",
    "def select_mode(key, mode):\n",
    "    number = -1\n",
    "    if 48 <= key <= 57:  # 0 ~ 9\n",
    "        number = key - 48\n",
    "    if key == 110:  # n\n",
    "        mode = 0\n",
    "    if key == 107:  # k\n",
    "        mode = 1\n",
    "    if key == 104:  # h\n",
    "        mode = 2\n",
    "    return number, mode\n",
    "\n",
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "\n",
    "def pre_process_point_history(image, point_history):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    temp_point_history = copy.deepcopy(point_history)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, point in enumerate(temp_point_history):\n",
    "        if index == 0:\n",
    "            base_x, base_y = point[0], point[1]\n",
    "\n",
    "        temp_point_history[index][0] = (temp_point_history[index][0] -\n",
    "                                        base_x) / image_width\n",
    "        temp_point_history[index][1] = (temp_point_history[index][1] -\n",
    "                                        base_y) / image_height\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_point_history = list(\n",
    "        itertools.chain.from_iterable(temp_point_history))\n",
    "\n",
    "    return temp_point_history\n",
    "\n",
    "\n",
    "def logging_csv(number, mode, landmark_list, point_history_list):\n",
    "    if mode == 0:\n",
    "        pass\n",
    "    if mode == 1 and (0 <= number <= 9):\n",
    "        csv_path = 'model/keypoint_classifier/keypoint.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *landmark_list])\n",
    "    if mode == 2 and (0 <= number <= 9):\n",
    "        csv_path = 'model/point_history_classifier/point_history.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *point_history_list])\n",
    "    return\n",
    "\n",
    "def draw_landmarks(image, landmark_point):\n",
    "    if len(landmark_point) > 0:\n",
    "        # Thumb\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Index finger\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Middle finger\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Ring finger\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Little finger\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Palm\n",
    "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "    # Key Points\n",
    "    for index, landmark in enumerate(landmark_point):\n",
    "        if index == 0:  # 手首1\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 1:  # 手首2\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 2:  # 親指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 3:  # 親指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 4:  # 親指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 5:  # 人差指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 6:  # 人差指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 7:  # 人差指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 8:  # 人差指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 9:  # 中指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 10:  # 中指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 11:  # 中指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 12:  # 中指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 13:  # 薬指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 14:  # 薬指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 15:  # 薬指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 16:  # 薬指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 17:  # 小指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 18:  # 小指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 19:  # 小指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 20:  # 小指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        # Outer rectangle\n",
    "        cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info_text(image, brect, handedness, hand_sign_text,\n",
    "                   finger_gesture_text):\n",
    "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                 (0, 0, 0), -1)\n",
    "\n",
    "    info_text = handedness.classification[0].label[0:]\n",
    "    if hand_sign_text != \"\":\n",
    "        info_text = info_text + ':' + hand_sign_text\n",
    "    cv2.putText(image, info_text, (brect[0] + 5, brect[1] - 4),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    if finger_gesture_text != \"\":\n",
    "        cv2.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
    "                   cv2.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_point_history(image, point_history):\n",
    "    for index, point in enumerate(point_history):\n",
    "        if point[0] != 0 and point[1] != 0:\n",
    "            cv2.circle(image, (point[0], point[1]), 1 + int(index / 2),\n",
    "                      (152, 251, 152), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info(image, fps, mode, number):\n",
    "    cv2.putText(image, \"FPS:\" + str(fps), (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "    cv2.putText(image, \"FPS:\" + str(fps), (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    mode_string = ['Logging Key Point', 'Logging Point History']\n",
    "    if 1 <= mode <= 2:\n",
    "        cv2.putText(image, \"MODE:\" + mode_string[mode - 1], (10, 90),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                   cv2.LINE_AA)\n",
    "        if 0 <= number <= 9:\n",
    "            cv2.putText(image, \"NUM:\" + str(number), (10, 110),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                       cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_single_camera(cam):\n",
    "    \"\"\"\n",
    "    This function acts as the body of the example; please see NodeMapInfo example\n",
    "    for more in-depth comments on setting up cameras.\n",
    "\n",
    "    :param cam: Camera to run on.\n",
    "    :type cam: CameraPtr\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = True\n",
    "\n",
    "        nodemap_tldevice = cam.GetTLDeviceNodeMap()\n",
    "\n",
    "        # Initialize camera\n",
    "        cam.Init()\n",
    "\n",
    "        # Retrieve GenICam nodemap\n",
    "        nodemap = cam.GetNodeMap()\n",
    "\n",
    "        # Acquire images\n",
    "        result &= acquire_and_display_images(cam, nodemap, nodemap_tldevice)\n",
    "\n",
    "        # Deinitialize camera\n",
    "        cam.DeInit()\n",
    "\n",
    "    except PySpin.SpinnakerException as ex:\n",
    "        print('Error: %s' % ex)\n",
    "        result = False\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example entry point; notice the volume of data that the logging event handler\n",
    "    prints out on debug despite the fact that very little really happens in this\n",
    "    example. Because of this, it may be better to have the logger set to lower\n",
    "    level in order to provide a more concise, focused log.\n",
    "\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    result = True\n",
    "\n",
    "    # Retrieve singleton reference to system object\n",
    "    system = PySpin.System.GetInstance()\n",
    "\n",
    "    # Get current library version\n",
    "    version = system.GetLibraryVersion()\n",
    "    print('Library version: %d.%d.%d.%d' % (version.major, version.minor, version.type, version.build))\n",
    "\n",
    "    # Retrieve list of cameras from the system\n",
    "    cam_list = system.GetCameras()\n",
    "\n",
    "    num_cameras = cam_list.GetSize()\n",
    "\n",
    "    print('Number of cameras detected: %d' % num_cameras)\n",
    "\n",
    "    # Finish if there are no cameras\n",
    "    if num_cameras == 0:\n",
    "\n",
    "        # Clear camera list before releasing system\n",
    "        cam_list.Clear()\n",
    "\n",
    "        # Release system instance\n",
    "        system.ReleaseInstance()\n",
    "\n",
    "        print('Not enough cameras!')\n",
    "        input('Done! Press Enter to exit...')\n",
    "        return False\n",
    "\n",
    "    # Run example on each camera\n",
    "    for i, cam in enumerate(cam_list):\n",
    "\n",
    "        print('Running example for camera %d...' % i)\n",
    "\n",
    "        result &= run_single_camera(cam)\n",
    "        print('Camera %d example complete... \\n' % i)\n",
    "\n",
    "    # Release reference to camera\n",
    "    # NOTE: Unlike the C++ examples, we cannot rely on pointer objects being automatically\n",
    "    # cleaned up when going out of scope.\n",
    "    # The usage of del is preferred to assigning the variable to None.\n",
    "    del cam\n",
    "\n",
    "    # Clear camera list before releasing system\n",
    "    cam_list.Clear()\n",
    "\n",
    "    # Release system instance\n",
    "    system.ReleaseInstance()\n",
    "\n",
    "    input('Done! Press Enter to exit...')\n",
    "    cv2.destroyAllWindows()\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if main():\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library version: 2.4.0.144\n",
      "Number of cameras detected: 1\n",
      "Running example for camera 0...\n",
      "*** IMAGE ACQUISITION ***\n",
      "\n",
      "Acquisition mode set to continuous...\n",
      "Acquiring images...\n",
      "Device serial number retrieved as 20304031...\n",
      "Press enter to close the program..\n",
      "Error: Spinnaker: Stream has been aborted. [-1012]\n",
      "Camera 0 example complete... \n",
      "\n",
      "Done! Press Enter to exit...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jana-\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Fast cam acquisition and MediaPipe HOLISTIC recognition AND gesture recognition\n",
    "\n",
    "\n",
    "# coding=utf-8\n",
    "# =============================================================================\n",
    "# Copyright (c) 2001-2021 FLIR Systems, Inc. All Rights Reserved.\n",
    "#\n",
    "# This software is the confidential and proprietary information of FLIR\n",
    "# Integrated Imaging Solutions, Inc. (\"Confidential Information\"). You\n",
    "# shall not disclose such Confidential Information and shall use it only in\n",
    "# accordance with the terms of the license agreement you entered into\n",
    "# with FLIR Integrated Imaging Solutions, Inc. (FLIR).\n",
    "#\n",
    "# FLIR MAKES NO REPRESENTATIONS OR WARRANTIES ABOUT THE SUITABILITY OF THE\n",
    "# SOFTWARE, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR\n",
    "# PURPOSE, OR NON-INFRINGEMENT. FLIR SHALL NOT BE LIABLE FOR ANY DAMAGES\n",
    "# SUFFERED BY LICENSEE AS A RESULT OF USING, MODIFYING OR DISTRIBUTING\n",
    "# THIS SOFTWARE OR ITS DERIVATIVES.\n",
    "# =============================================================================\n",
    "#\n",
    "# This AcquireAndDisplay.py shows how to get the image data, and then display images in a GUI.\n",
    "# This example relies on information provided in the ImageChannelStatistics.py example.\n",
    "#\n",
    "# This example demonstrates how to display images represented as numpy arrays.\n",
    "# Currently, this program is limited to single camera use.\n",
    "# NOTE: keyboard and matplotlib must be installed on Python interpreter prior to running this example.\n",
    "\n",
    "import os\n",
    "from pyspin import PySpin\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import keyboard\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Mediapipe hand tracking and OpenCV and Pillow\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "## gesture recognition\n",
    "from utils import CvFpsCalc\n",
    "from model import KeyPointClassifier\n",
    "from model import PointHistoryClassifier\n",
    "\n",
    "import csv\n",
    "import copy\n",
    "import argparse\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "global continue_recording\n",
    "continue_recording = True\n",
    "\n",
    "\n",
    "def handle_close(evt):\n",
    "    \"\"\"\n",
    "    This function will close the GUI when close event happens.\n",
    "\n",
    "    :param evt: Event that occurs when the figure closes.\n",
    "    :type evt: Event\n",
    "    \"\"\"\n",
    "\n",
    "    global continue_recording\n",
    "    continue_recording = False\n",
    "\n",
    "\n",
    "def acquire_and_display_images(cam, nodemap, nodemap_tldevice):\n",
    "    \"\"\"\n",
    "    This function continuously acquires images from a device and display them in a GUI.\n",
    "\n",
    "    :param cam: Camera to acquire images from.\n",
    "    :param nodemap: Device nodemap.\n",
    "    :param nodemap_tldevice: Transport layer device nodemap.\n",
    "    :type cam: CameraPtr\n",
    "    :type nodemap: INodeMap\n",
    "    :type nodemap_tldevice: INodeMap\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    global continue_recording\n",
    "\n",
    "    sNodemap = cam.GetTLStreamNodeMap()\n",
    "\n",
    "    # Change bufferhandling mode to NewestOnly\n",
    "    node_bufferhandling_mode = PySpin.CEnumerationPtr(sNodemap.GetNode('StreamBufferHandlingMode'))\n",
    "    if not PySpin.IsAvailable(node_bufferhandling_mode) or not PySpin.IsWritable(node_bufferhandling_mode):\n",
    "        print('Unable to set stream buffer handling mode.. Aborting...')\n",
    "        return False\n",
    "\n",
    "    # Retrieve entry node from enumeration node\n",
    "    node_newestonly = node_bufferhandling_mode.GetEntryByName('NewestOnly')\n",
    "    if not PySpin.IsAvailable(node_newestonly) or not PySpin.IsReadable(node_newestonly):\n",
    "        print('Unable to set stream buffer handling mode.. Aborting...')\n",
    "        return False\n",
    "\n",
    "    # Retrieve integer value from entry node\n",
    "    node_newestonly_mode = node_newestonly.GetValue()\n",
    "\n",
    "    # Set integer value from entry node as new value of enumeration node\n",
    "    node_bufferhandling_mode.SetIntValue(node_newestonly_mode)\n",
    "\n",
    "    print('*** IMAGE ACQUISITION ***\\n')\n",
    "    try:\n",
    "        ## gesture recognition\n",
    "        result = True\n",
    "        use_brect = True\n",
    "        \n",
    "        ## media pipe\n",
    "        node_acquisition_mode = PySpin.CEnumerationPtr(nodemap.GetNode('AcquisitionMode'))\n",
    "        if not PySpin.IsAvailable(node_acquisition_mode) or not PySpin.IsWritable(node_acquisition_mode):\n",
    "            print('Unable to set acquisition mode to continuous (enum retrieval). Aborting...')\n",
    "            return False\n",
    "\n",
    "        # Retrieve entry node from enumeration node\n",
    "        node_acquisition_mode_continuous = node_acquisition_mode.GetEntryByName('Continuous')\n",
    "        if not PySpin.IsAvailable(node_acquisition_mode_continuous) or not PySpin.IsReadable(\n",
    "                node_acquisition_mode_continuous):\n",
    "            print('Unable to set acquisition mode to continuous (entry retrieval). Aborting...')\n",
    "            return False\n",
    "\n",
    "        # Retrieve integer value from entry node\n",
    "        acquisition_mode_continuous = node_acquisition_mode_continuous.GetValue()\n",
    "\n",
    "        # Set integer value from entry node as new value of enumeration node\n",
    "        node_acquisition_mode.SetIntValue(acquisition_mode_continuous)\n",
    "\n",
    "        print('Acquisition mode set to continuous...')\n",
    "\n",
    "        #  Begin acquiring images\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  What happens when the camera begins acquiring images depends on the\n",
    "        #  acquisition mode. Single frame captures only a single image, multi\n",
    "        #  frame catures a set number of images, and continuous captures a\n",
    "        #  continuous stream of images.\n",
    "        #\n",
    "        #  *** LATER ***\n",
    "        #  Image acquisition must be ended when no more images are needed.\n",
    "        cam.BeginAcquisition()\n",
    "\n",
    "        print('Acquiring images...')\n",
    "\n",
    "        #  Retrieve device serial number for filename\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  The device serial number is retrieved in order to keep cameras from\n",
    "        #  overwriting one another. Grabbing image IDs could also accomplish\n",
    "        #  this.\n",
    "        device_serial_number = ''\n",
    "        node_device_serial_number = PySpin.CStringPtr(nodemap_tldevice.GetNode('DeviceSerialNumber'))\n",
    "        if PySpin.IsAvailable(node_device_serial_number) and PySpin.IsReadable(node_device_serial_number):\n",
    "            device_serial_number = node_device_serial_number.GetValue()\n",
    "            print('Device serial number retrieved as %s...' % device_serial_number)\n",
    "\n",
    "        # Close program\n",
    "        print('Press enter to close the program..')\n",
    "\n",
    "        # Figure(1) is default so you can omit this line. Figure(0) will create a new window every time program hits this line\n",
    "        fig = plt.figure(1)\n",
    "\n",
    "        # Close the GUI when close event happens\n",
    "        fig.canvas.mpl_connect('close_event', handle_close)\n",
    "\n",
    "        \n",
    "        ## this is for the pose aquisition\n",
    "        with mp_holistic.Holistic(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        \n",
    "            ## this is the hand detection loop (maybe I can later feed the cropped hands from the pose loop only?)\n",
    "            # Retrieve and display images\n",
    "            with mp_hands.Hands(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "                ## for gesture recognition\n",
    "                keypoint_classifier = KeyPointClassifier()\n",
    "\n",
    "                point_history_classifier = PointHistoryClassifier()\n",
    "\n",
    "                # Read labels ###########################################################\n",
    "                with open('model/keypoint_classifier/keypoint_classifier_label.csv', encoding='utf-8-sig') as f:\n",
    "                    keypoint_classifier_labels = csv.reader(f)\n",
    "                    keypoint_classifier_labels = [row[0] for row in keypoint_classifier_labels]\n",
    "                with open('model/point_history_classifier/point_history_classifier_label.csv', encoding='utf-8-sig') as f:\n",
    "                    point_history_classifier_labels = csv.reader(f)\n",
    "                    point_history_classifier_labels = [row[0] for row in point_history_classifier_labels]\n",
    "\n",
    "                # FPS Measurement ########################################################\n",
    "                cvFpsCalc = CvFpsCalc(buffer_len=10)\n",
    "\n",
    "                # Coordinate history #################################################################\n",
    "                history_length = 16\n",
    "                point_history = deque(maxlen=history_length)\n",
    "\n",
    "                # Finger gesture history ################################################\n",
    "                finger_gesture_history = deque(maxlen=history_length)\n",
    "\n",
    "                #  ########################################################################\n",
    "\n",
    "                mode = 0\n",
    "\n",
    "                while(continue_recording):\n",
    "                    try:\n",
    "\n",
    "\n",
    "                        fps = cvFpsCalc.get()\n",
    "                        # Process Key (ESC: end) #################################################\n",
    "                        key = cv2.waitKey(1)\n",
    "                        if key == 27:  # ESC\n",
    "                            break\n",
    "                        number, mode = select_mode(key, mode)\n",
    "\n",
    "                        #  Retrieve next received image\n",
    "                        #\n",
    "                        #  *** NOTES ***\n",
    "                        #  Capturing an image houses images on the camera buffer. Trying\n",
    "                        #  to capture an image that does not exist will hang the camera.\n",
    "                        #\n",
    "                        #  *** LATER ***\n",
    "                        #  Once an image from the buffer is saved and/or no longer\n",
    "                        #  needed, the image must be released in order to keep the\n",
    "                        #  buffer from filling up.\n",
    "\n",
    "                        image_result = cam.GetNextImage(1000)\n",
    "\n",
    "                        #  Ensure image completion\n",
    "                        if image_result.IsIncomplete():\n",
    "                            print('Image incomplete with image status %d ...' % image_result.GetImageStatus())\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            ## This is 2048x2048 so we need to crop it to the ROI from the lense\n",
    "                            width = image_result.GetWidth()\n",
    "                            height = image_result.GetHeight()\n",
    "\n",
    "\n",
    "                            ## This converts it to GreyScale\n",
    "                            #image_converted = image_result.Convert(spin.PixelFormat_Mono8, spin.HQ_LINEAR)\n",
    "                            ## This converts it to RGB\n",
    "                            image_converted = image_result.Convert(PySpin.PixelFormat_BGR8)\n",
    "                            rgb_array = image_converted.GetData()\n",
    "                            rgb_array = rgb_array.reshape(height, width, 3)\n",
    "\n",
    "\n",
    "                            ## process mediapipe on image\n",
    "                            #image_rgb = cv2.cvtColor(cv2.flip(rgb_array, 1), cv2.COLOR_BGR2RGB)\n",
    "                            image_rgb = cv2.flip(rgb_array, 1)\n",
    "\n",
    "\n",
    "                            ## **** Resizing / Croping *****\n",
    "\n",
    "                            ## RESIZING the image since it would be 2048x2048 otherwise (kind of too big for the window)\n",
    "\n",
    "                            ## we might have to size this further down for mediapipe to run fast\n",
    "                            #image_rgb = cv2.resize(image_rgb, (800, 800))\n",
    "                            # scale = 800/2048   \n",
    "                            ## this is to display potential cropping region in the downscaled image\n",
    "                            # cv2.rectangle(image_rgb, (int(650*scale), int(580*scale)), (int(1450*scale), int(1380*scale)), (0, 255, 0), 3)\n",
    "\n",
    "                            ## CROPPPING the region of the lense (should be around 800 to fit with the setup so far...)\n",
    "                            ## This is the cropping\n",
    "                            ## These values are taken from the unity config\n",
    "                            ## They might change...\n",
    "                            #array_cropped = image_rgb[ROI_y:(ROI_y + ROI_height), ROI_x:(ROI_x + ROI_width)]\n",
    "                            #image_rgb = array_cropped.copy() # needed to get the correct data format for further processing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            ## **** un-distort image\n",
    "\n",
    "                            #undistorted_img = cv2.remap(image_rgb, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "                            ## media pipe works better on rgb image\n",
    "                            image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB)\n",
    "                            image_rgb = cv2.resize(image_rgb, (800, 800))\n",
    "\n",
    "\n",
    "                            ## **** Mediapipe Holistic Detection ****\n",
    "                            holisticResults = holistic.process(image_rgb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            ## **** Mediapipe hand detection ****\n",
    "                            image_rgb.flags.writeable = False\n",
    "                            results = hands.process(image_rgb)\n",
    "                            image_rgb.flags.writeable = True\n",
    "\n",
    "                            ## convert image back to bgr for outputting it in rgb with the cam (python is confusing...)\n",
    "                            image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                            # Draw the hand annotations on the image.\n",
    "                            #if results.multi_hand_landmarks:\n",
    "                            #    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                            #        mp_drawing.draw_landmarks(\n",
    "                            #            image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            #gesture recongition output and pose recognition output\n",
    "                            debug_image = copy.deepcopy(image_rgb)\n",
    "                            holistic_image = copy.deepcopy(image_rgb)\n",
    "\n",
    "\n",
    "                            ## Draw the holistic annotations on the image\n",
    "                            mp_drawing.draw_landmarks(\n",
    "                                holistic_image,\n",
    "                                holisticResults.face_landmarks,\n",
    "                                mp_holistic.FACEMESH_CONTOURS,\n",
    "                                landmark_drawing_spec=None,\n",
    "                                connection_drawing_spec=mp_drawing_styles\n",
    "                                .get_default_face_mesh_contours_style())\n",
    "                            mp_drawing.draw_landmarks(\n",
    "                                holistic_image,\n",
    "                                holisticResults.pose_landmarks,\n",
    "                                mp_holistic.POSE_CONNECTIONS,\n",
    "                                landmark_drawing_spec=mp_drawing_styles\n",
    "                                .get_default_pose_landmarks_style())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            # Draw the hand annotations on the image.\n",
    "                            if results.multi_hand_landmarks:\n",
    "                                for hand_landmarks, handedness in zip(results.multi_hand_landmarks,results.multi_handedness):\n",
    "                                    mp_drawing.draw_landmarks(image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                                    ## gesture recognition\n",
    "\n",
    "                                    # Bounding box calculation\n",
    "                                    brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
    "                                    # Landmark calculation\n",
    "                                    landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "                                    ## Landmark calcualtion with 3 coordinates\n",
    "                                    hand_landmarks_list = get_hand_landmarks_list(debug_image, hand_landmarks)\n",
    "\n",
    "                                    # Conversion to relative coordinates / normalized coordinates\n",
    "                                    pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "                                    pre_processed_point_history_list = pre_process_point_history(debug_image, point_history)\n",
    "                                    # Write to the dataset file\n",
    "                                    logging_csv(number, mode, pre_processed_landmark_list, pre_processed_point_history_list)\n",
    "\n",
    "                                    # Hand sign classification\n",
    "                                    hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "                                    if hand_sign_id == 2:  # Point gesture\n",
    "                                        point_history.append(landmark_list[8])\n",
    "                                    else:\n",
    "                                        point_history.append([0, 0])\n",
    "\n",
    "                                    # Finger gesture classification\n",
    "                                    finger_gesture_id = 0\n",
    "                                    point_history_len = len(pre_processed_point_history_list)\n",
    "                                    if point_history_len == (history_length * 2):\n",
    "                                        finger_gesture_id = point_history_classifier(pre_processed_point_history_list)\n",
    "\n",
    "                                    # Calculates the gesture IDs in the latest detection\n",
    "                                    finger_gesture_history.append(finger_gesture_id)\n",
    "                                    most_common_fg_id = Counter(finger_gesture_history).most_common()\n",
    "\n",
    "                                    # Drawing part\n",
    "                                    debug_image = draw_bounding_rect(use_brect, debug_image, brect)\n",
    "                                    debug_image = draw_landmarks(debug_image, landmark_list)\n",
    "                                    debug_image = draw_info_text(\n",
    "                                        debug_image,\n",
    "                                        brect,\n",
    "                                        handedness,\n",
    "                                        keypoint_classifier_labels[hand_sign_id],\n",
    "                                        point_history_classifier_labels[most_common_fg_id[0][0]],\n",
    "                                    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            cv2.imshow('MediaPipe Holistic', holistic_image)         \n",
    "                            cv2.imshow('Hand Gesture Recognition', debug_image)    \n",
    "                            cv2.imshow('MediaPipe Hands', image_rgb)\n",
    "                            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                                break\n",
    "                                \n",
    "                        #image_result.Release()\n",
    "\n",
    "                        # Getting the image data as a numpy array\n",
    "                        #image_data = image_result.GetNDArray()\n",
    "\n",
    "                        # Draws an image on the current figure\n",
    "                        #plt.imshow(image_data, cmap='gray')\n",
    "\n",
    "                        # Interval in plt.pause(interval) determines how fast the images are displayed in a GUI\n",
    "                        # Interval is in seconds.\n",
    "                        #plt.pause(0.001)\n",
    "\n",
    "                        # Clear current reference of a figure. This will improve display speed significantly\n",
    "                        #plt.clf()\n",
    "\n",
    "                        # If user presses enter, close the program\n",
    "                        if keyboard.is_pressed('ENTER'):\n",
    "                            print('Program is closing...')\n",
    "\n",
    "                            # Close figure\n",
    "                            plt.close('all')             \n",
    "                            input('Done! Press Enter to exit...')\n",
    "                            continue_recording=False                        \n",
    "\n",
    "                        #  Release image\n",
    "                        #\n",
    "                        #  *** NOTES ***\n",
    "                        #  Images retrieved directly from the camera (i.e. non-converted\n",
    "                        #  images) need to be released in order to keep from filling the\n",
    "                        #  buffer.\n",
    "                        image_result.Release()\n",
    "\n",
    "                    except PySpin.SpinnakerException as ex:\n",
    "                        print('Error: %s' % ex)\n",
    "                        return False\n",
    "\n",
    "        #  End acquisition\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  Ending acquisition appropriately helps ensure that devices clean up\n",
    "        #  properly and do not need to be power-cycled to maintain integrity.\n",
    "        cam.EndAcquisition()\n",
    "\n",
    "    except PySpin.SpinnakerException as ex:\n",
    "        print('Error: %s' % ex)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gesture recognition\n",
    "\n",
    "## converting hand_landmarks to sendable vector3 string list?\n",
    "## returns a list of all handmarks and their 3 coordinates (normalized)\n",
    "def get_hand_landmarks_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = landmark.x\n",
    "        landmark_y = landmark.y\n",
    "        landmark_z = landmark.z\n",
    "        #landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        #landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        #landmark_z = landmark.z\n",
    "\n",
    "        \n",
    "        ## need the brackets for this function\n",
    "        landmark_point.append([landmark_x, landmark_y, landmark_z])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gesture recognition functions\n",
    "\n",
    "def select_mode(key, mode):\n",
    "    number = -1\n",
    "    if 48 <= key <= 57:  # 0 ~ 9\n",
    "        number = key - 48\n",
    "    if key == 110:  # n\n",
    "        mode = 0\n",
    "    if key == 107:  # k\n",
    "        mode = 1\n",
    "    if key == 104:  # h\n",
    "        mode = 2\n",
    "    return number, mode\n",
    "\n",
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "\n",
    "def pre_process_point_history(image, point_history):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    temp_point_history = copy.deepcopy(point_history)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, point in enumerate(temp_point_history):\n",
    "        if index == 0:\n",
    "            base_x, base_y = point[0], point[1]\n",
    "\n",
    "        temp_point_history[index][0] = (temp_point_history[index][0] -\n",
    "                                        base_x) / image_width\n",
    "        temp_point_history[index][1] = (temp_point_history[index][1] -\n",
    "                                        base_y) / image_height\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_point_history = list(\n",
    "        itertools.chain.from_iterable(temp_point_history))\n",
    "\n",
    "    return temp_point_history\n",
    "\n",
    "\n",
    "def logging_csv(number, mode, landmark_list, point_history_list):\n",
    "    if mode == 0:\n",
    "        pass\n",
    "    if mode == 1 and (0 <= number <= 9):\n",
    "        csv_path = 'model/keypoint_classifier/keypoint.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *landmark_list])\n",
    "    if mode == 2 and (0 <= number <= 9):\n",
    "        csv_path = 'model/point_history_classifier/point_history.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *point_history_list])\n",
    "    return\n",
    "\n",
    "def draw_landmarks(image, landmark_point):\n",
    "    if len(landmark_point) > 0:\n",
    "        # Thumb\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Index finger\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Middle finger\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Ring finger\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Little finger\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Palm\n",
    "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "    # Key Points\n",
    "    for index, landmark in enumerate(landmark_point):\n",
    "        if index == 0:  # 手首1\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 1:  # 手首2\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 2:  # 親指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 3:  # 親指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 4:  # 親指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 5:  # 人差指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 6:  # 人差指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 7:  # 人差指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 8:  # 人差指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 9:  # 中指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 10:  # 中指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 11:  # 中指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 12:  # 中指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 13:  # 薬指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 14:  # 薬指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 15:  # 薬指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 16:  # 薬指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 17:  # 小指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 18:  # 小指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 19:  # 小指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 20:  # 小指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        # Outer rectangle\n",
    "        cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info_text(image, brect, handedness, hand_sign_text,\n",
    "                   finger_gesture_text):\n",
    "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                 (0, 0, 0), -1)\n",
    "\n",
    "    info_text = handedness.classification[0].label[0:]\n",
    "    if hand_sign_text != \"\":\n",
    "        info_text = info_text + ':' + hand_sign_text\n",
    "    cv2.putText(image, info_text, (brect[0] + 5, brect[1] - 4),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    if finger_gesture_text != \"\":\n",
    "        cv2.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
    "                   cv2.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_point_history(image, point_history):\n",
    "    for index, point in enumerate(point_history):\n",
    "        if point[0] != 0 and point[1] != 0:\n",
    "            cv2.circle(image, (point[0], point[1]), 1 + int(index / 2),\n",
    "                      (152, 251, 152), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info(image, fps, mode, number):\n",
    "    cv2.putText(image, \"FPS:\" + str(fps), (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "    cv2.putText(image, \"FPS:\" + str(fps), (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    mode_string = ['Logging Key Point', 'Logging Point History']\n",
    "    if 1 <= mode <= 2:\n",
    "        cv2.putText(image, \"MODE:\" + mode_string[mode - 1], (10, 90),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                   cv2.LINE_AA)\n",
    "        if 0 <= number <= 9:\n",
    "            cv2.putText(image, \"NUM:\" + str(number), (10, 110),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                       cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_single_camera(cam):\n",
    "    \"\"\"\n",
    "    This function acts as the body of the example; please see NodeMapInfo example\n",
    "    for more in-depth comments on setting up cameras.\n",
    "\n",
    "    :param cam: Camera to run on.\n",
    "    :type cam: CameraPtr\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = True\n",
    "\n",
    "        nodemap_tldevice = cam.GetTLDeviceNodeMap()\n",
    "\n",
    "        # Initialize camera\n",
    "        cam.Init()\n",
    "\n",
    "        # Retrieve GenICam nodemap\n",
    "        nodemap = cam.GetNodeMap()\n",
    "\n",
    "        # Acquire images\n",
    "        result &= acquire_and_display_images(cam, nodemap, nodemap_tldevice)\n",
    "\n",
    "        # Deinitialize camera\n",
    "        cam.DeInit()\n",
    "\n",
    "    except PySpin.SpinnakerException as ex:\n",
    "        print('Error: %s' % ex)\n",
    "        result = False\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example entry point; notice the volume of data that the logging event handler\n",
    "    prints out on debug despite the fact that very little really happens in this\n",
    "    example. Because of this, it may be better to have the logger set to lower\n",
    "    level in order to provide a more concise, focused log.\n",
    "\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    result = True\n",
    "\n",
    "    # Retrieve singleton reference to system object\n",
    "    system = PySpin.System.GetInstance()\n",
    "\n",
    "    # Get current library version\n",
    "    version = system.GetLibraryVersion()\n",
    "    print('Library version: %d.%d.%d.%d' % (version.major, version.minor, version.type, version.build))\n",
    "\n",
    "    # Retrieve list of cameras from the system\n",
    "    cam_list = system.GetCameras()\n",
    "\n",
    "    num_cameras = cam_list.GetSize()\n",
    "\n",
    "    print('Number of cameras detected: %d' % num_cameras)\n",
    "\n",
    "    # Finish if there are no cameras\n",
    "    if num_cameras == 0:\n",
    "\n",
    "        # Clear camera list before releasing system\n",
    "        cam_list.Clear()\n",
    "\n",
    "        # Release system instance\n",
    "        system.ReleaseInstance()\n",
    "\n",
    "        print('Not enough cameras!')\n",
    "        input('Done! Press Enter to exit...')\n",
    "        return False\n",
    "\n",
    "    # Run example on each camera\n",
    "    for i, cam in enumerate(cam_list):\n",
    "\n",
    "        print('Running example for camera %d...' % i)\n",
    "\n",
    "        result &= run_single_camera(cam)\n",
    "        print('Camera %d example complete... \\n' % i)\n",
    "\n",
    "    # Release reference to camera\n",
    "    # NOTE: Unlike the C++ examples, we cannot rely on pointer objects being automatically\n",
    "    # cleaned up when going out of scope.\n",
    "    # The usage of del is preferred to assigning the variable to None.\n",
    "    del cam\n",
    "\n",
    "    # Clear camera list before releasing system\n",
    "    cam_list.Clear()\n",
    "\n",
    "    # Release system instance\n",
    "    system.ReleaseInstance()\n",
    "\n",
    "    input('Done! Press Enter to exit...')\n",
    "    cv2.destroyAllWindows()\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if main():\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library version: 2.4.0.144\n",
      "Number of cameras detected: 1\n",
      "Running example for camera 0...\n",
      "*** IMAGE ACQUISITION ***\n",
      "\n",
      "Acquisition mode set to continuous...\n",
      "Acquiring images...\n",
      "Device serial number retrieved as 20304031...\n",
      "Press enter to close the program..\n",
      "Camera 0 example complete... \n",
      "\n",
      "Done! Press Enter to exit...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Fast cam acquisition and MediaPipe HOLISTIC recognition WITHOUT gesture recognition\n",
    "## (since gesture rec should already be in holistic)\n",
    "\n",
    "\n",
    "# coding=utf-8\n",
    "# =============================================================================\n",
    "# Copyright (c) 2001-2021 FLIR Systems, Inc. All Rights Reserved.\n",
    "#\n",
    "# This software is the confidential and proprietary information of FLIR\n",
    "# Integrated Imaging Solutions, Inc. (\"Confidential Information\"). You\n",
    "# shall not disclose such Confidential Information and shall use it only in\n",
    "# accordance with the terms of the license agreement you entered into\n",
    "# with FLIR Integrated Imaging Solutions, Inc. (FLIR).\n",
    "#\n",
    "# FLIR MAKES NO REPRESENTATIONS OR WARRANTIES ABOUT THE SUITABILITY OF THE\n",
    "# SOFTWARE, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR\n",
    "# PURPOSE, OR NON-INFRINGEMENT. FLIR SHALL NOT BE LIABLE FOR ANY DAMAGES\n",
    "# SUFFERED BY LICENSEE AS A RESULT OF USING, MODIFYING OR DISTRIBUTING\n",
    "# THIS SOFTWARE OR ITS DERIVATIVES.\n",
    "# =============================================================================\n",
    "#\n",
    "# This AcquireAndDisplay.py shows how to get the image data, and then display images in a GUI.\n",
    "# This example relies on information provided in the ImageChannelStatistics.py example.\n",
    "#\n",
    "# This example demonstrates how to display images represented as numpy arrays.\n",
    "# Currently, this program is limited to single camera use.\n",
    "# NOTE: keyboard and matplotlib must be installed on Python interpreter prior to running this example.\n",
    "\n",
    "import os\n",
    "from pyspin import PySpin\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import keyboard\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Mediapipe hand tracking and OpenCV and Pillow\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "#mp_hands = mp.solutions.hands\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "## gesture recognition\n",
    "from utils import CvFpsCalc\n",
    "from model import KeyPointClassifier\n",
    "from model import PointHistoryClassifier\n",
    "\n",
    "import csv\n",
    "import copy\n",
    "import argparse\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "global continue_recording\n",
    "continue_recording = True\n",
    "\n",
    "\n",
    "def handle_close(evt):\n",
    "    \"\"\"\n",
    "    This function will close the GUI when close event happens.\n",
    "\n",
    "    :param evt: Event that occurs when the figure closes.\n",
    "    :type evt: Event\n",
    "    \"\"\"\n",
    "\n",
    "    global continue_recording\n",
    "    continue_recording = False\n",
    "\n",
    "\n",
    "def acquire_and_display_images(cam, nodemap, nodemap_tldevice):\n",
    "    \"\"\"\n",
    "    This function continuously acquires images from a device and display them in a GUI.\n",
    "\n",
    "    :param cam: Camera to acquire images from.\n",
    "    :param nodemap: Device nodemap.\n",
    "    :param nodemap_tldevice: Transport layer device nodemap.\n",
    "    :type cam: CameraPtr\n",
    "    :type nodemap: INodeMap\n",
    "    :type nodemap_tldevice: INodeMap\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    global continue_recording\n",
    "\n",
    "    sNodemap = cam.GetTLStreamNodeMap()\n",
    "\n",
    "    # Change bufferhandling mode to NewestOnly\n",
    "    node_bufferhandling_mode = PySpin.CEnumerationPtr(sNodemap.GetNode('StreamBufferHandlingMode'))\n",
    "    if not PySpin.IsAvailable(node_bufferhandling_mode) or not PySpin.IsWritable(node_bufferhandling_mode):\n",
    "        print('Unable to set stream buffer handling mode.. Aborting...')\n",
    "        return False\n",
    "\n",
    "    # Retrieve entry node from enumeration node\n",
    "    node_newestonly = node_bufferhandling_mode.GetEntryByName('NewestOnly')\n",
    "    if not PySpin.IsAvailable(node_newestonly) or not PySpin.IsReadable(node_newestonly):\n",
    "        print('Unable to set stream buffer handling mode.. Aborting...')\n",
    "        return False\n",
    "\n",
    "    # Retrieve integer value from entry node\n",
    "    node_newestonly_mode = node_newestonly.GetValue()\n",
    "\n",
    "    # Set integer value from entry node as new value of enumeration node\n",
    "    node_bufferhandling_mode.SetIntValue(node_newestonly_mode)\n",
    "\n",
    "    print('*** IMAGE ACQUISITION ***\\n')\n",
    "    try:\n",
    "        ## gesture recognition\n",
    "        result = True\n",
    "        use_brect = True\n",
    "        \n",
    "        ## media pipe\n",
    "        node_acquisition_mode = PySpin.CEnumerationPtr(nodemap.GetNode('AcquisitionMode'))\n",
    "        if not PySpin.IsAvailable(node_acquisition_mode) or not PySpin.IsWritable(node_acquisition_mode):\n",
    "            print('Unable to set acquisition mode to continuous (enum retrieval). Aborting...')\n",
    "            return False\n",
    "\n",
    "        # Retrieve entry node from enumeration node\n",
    "        node_acquisition_mode_continuous = node_acquisition_mode.GetEntryByName('Continuous')\n",
    "        if not PySpin.IsAvailable(node_acquisition_mode_continuous) or not PySpin.IsReadable(\n",
    "                node_acquisition_mode_continuous):\n",
    "            print('Unable to set acquisition mode to continuous (entry retrieval). Aborting...')\n",
    "            return False\n",
    "\n",
    "        # Retrieve integer value from entry node\n",
    "        acquisition_mode_continuous = node_acquisition_mode_continuous.GetValue()\n",
    "\n",
    "        # Set integer value from entry node as new value of enumeration node\n",
    "        node_acquisition_mode.SetIntValue(acquisition_mode_continuous)\n",
    "\n",
    "        print('Acquisition mode set to continuous...')\n",
    "\n",
    "        #  Begin acquiring images\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  What happens when the camera begins acquiring images depends on the\n",
    "        #  acquisition mode. Single frame captures only a single image, multi\n",
    "        #  frame catures a set number of images, and continuous captures a\n",
    "        #  continuous stream of images.\n",
    "        #\n",
    "        #  *** LATER ***\n",
    "        #  Image acquisition must be ended when no more images are needed.\n",
    "        cam.BeginAcquisition()\n",
    "\n",
    "        print('Acquiring images...')\n",
    "\n",
    "        #  Retrieve device serial number for filename\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  The device serial number is retrieved in order to keep cameras from\n",
    "        #  overwriting one another. Grabbing image IDs could also accomplish\n",
    "        #  this.\n",
    "        device_serial_number = ''\n",
    "        node_device_serial_number = PySpin.CStringPtr(nodemap_tldevice.GetNode('DeviceSerialNumber'))\n",
    "        if PySpin.IsAvailable(node_device_serial_number) and PySpin.IsReadable(node_device_serial_number):\n",
    "            device_serial_number = node_device_serial_number.GetValue()\n",
    "            print('Device serial number retrieved as %s...' % device_serial_number)\n",
    "\n",
    "        # Close program\n",
    "        print('Press enter to close the program..')\n",
    "\n",
    "        # Figure(1) is default so you can omit this line. Figure(0) will create a new window every time program hits this line\n",
    "        fig = plt.figure(1)\n",
    "\n",
    "        # Close the GUI when close event happens\n",
    "        fig.canvas.mpl_connect('close_event', handle_close)\n",
    "\n",
    "        \n",
    "        ## this is for the pose aquisition\n",
    "        with mp_holistic.Holistic(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        \n",
    "            ## this is the hand detection loop (maybe I can later feed the cropped hands from the pose loop only?)\n",
    "            # Retrieve and display images\n",
    "#             with mp_hands.Hands(\n",
    "#             min_detection_confidence=0.5,\n",
    "#             min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "                ## for gesture recognition\n",
    "#                 keypoint_classifier = KeyPointClassifier()\n",
    "\n",
    "#                 point_history_classifier = PointHistoryClassifier()\n",
    "\n",
    "#                 # Read labels ###########################################################\n",
    "#                 with open('model/keypoint_classifier/keypoint_classifier_label.csv', encoding='utf-8-sig') as f:\n",
    "#                     keypoint_classifier_labels = csv.reader(f)\n",
    "#                     keypoint_classifier_labels = [row[0] for row in keypoint_classifier_labels]\n",
    "#                 with open('model/point_history_classifier/point_history_classifier_label.csv', encoding='utf-8-sig') as f:\n",
    "#                     point_history_classifier_labels = csv.reader(f)\n",
    "#                     point_history_classifier_labels = [row[0] for row in point_history_classifier_labels]\n",
    "\n",
    "#                 # FPS Measurement ########################################################\n",
    "#                 cvFpsCalc = CvFpsCalc(buffer_len=10)\n",
    "\n",
    "#                 # Coordinate history #################################################################\n",
    "#                 history_length = 16\n",
    "#                 point_history = deque(maxlen=history_length)\n",
    "\n",
    "#                 # Finger gesture history ################################################\n",
    "#                 finger_gesture_history = deque(maxlen=history_length)\n",
    "\n",
    "                #  ########################################################################\n",
    "\n",
    "            mode = 0\n",
    "\n",
    "            while(continue_recording):\n",
    "                try:\n",
    "\n",
    "\n",
    "                    #fps = cvFpsCalc.get()\n",
    "                    # Process Key (ESC: end) #################################################\n",
    "                    key = cv2.waitKey(1)\n",
    "                    if key == 27:  # ESC\n",
    "                        break\n",
    "                    number, mode = select_mode(key, mode)\n",
    "\n",
    "                    #  Retrieve next received image\n",
    "                    #\n",
    "                    #  *** NOTES ***\n",
    "                    #  Capturing an image houses images on the camera buffer. Trying\n",
    "                    #  to capture an image that does not exist will hang the camera.\n",
    "                    #\n",
    "                    #  *** LATER ***\n",
    "                    #  Once an image from the buffer is saved and/or no longer\n",
    "                    #  needed, the image must be released in order to keep the\n",
    "                    #  buffer from filling up.\n",
    "\n",
    "                    image_result = cam.GetNextImage(1000)\n",
    "\n",
    "                    #  Ensure image completion\n",
    "                    if image_result.IsIncomplete():\n",
    "                        print('Image incomplete with image status %d ...' % image_result.GetImageStatus())\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        ## This is 2048x2048 so we need to crop it to the ROI from the lense\n",
    "                        width = image_result.GetWidth()\n",
    "                        height = image_result.GetHeight()\n",
    "\n",
    "\n",
    "                        ## This converts it to GreyScale\n",
    "                        #image_converted = image_result.Convert(spin.PixelFormat_Mono8, spin.HQ_LINEAR)\n",
    "                        ## This converts it to RGB\n",
    "                        image_converted = image_result.Convert(PySpin.PixelFormat_BGR8)\n",
    "                        rgb_array = image_converted.GetData()\n",
    "                        rgb_array = rgb_array.reshape(height, width, 3)\n",
    "\n",
    "\n",
    "                        ## process mediapipe on image\n",
    "                        #image_rgb = cv2.cvtColor(cv2.flip(rgb_array, 1), cv2.COLOR_BGR2RGB)\n",
    "                        image_rgb = cv2.flip(rgb_array, 1)\n",
    "\n",
    "\n",
    "                        ## **** Resizing / Croping *****\n",
    "\n",
    "                        ## RESIZING the image since it would be 2048x2048 otherwise (kind of too big for the window)\n",
    "\n",
    "                        ## we might have to size this further down for mediapipe to run fast\n",
    "                        #image_rgb = cv2.resize(image_rgb, (800, 800))\n",
    "                        # scale = 800/2048   \n",
    "                        ## this is to display potential cropping region in the downscaled image\n",
    "                        # cv2.rectangle(image_rgb, (int(650*scale), int(580*scale)), (int(1450*scale), int(1380*scale)), (0, 255, 0), 3)\n",
    "\n",
    "                        ## CROPPPING the region of the lense (should be around 800 to fit with the setup so far...)\n",
    "                        ## This is the cropping\n",
    "                        ## These values are taken from the unity config\n",
    "                        ## They might change...\n",
    "                        #array_cropped = image_rgb[ROI_y:(ROI_y + ROI_height), ROI_x:(ROI_x + ROI_width)]\n",
    "                        #image_rgb = array_cropped.copy() # needed to get the correct data format for further processing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        ## **** un-distort image\n",
    "\n",
    "                        #undistorted_img = cv2.remap(image_rgb, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "\n",
    "                        ## media pipe works better on rgb image\n",
    "                        image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB)\n",
    "                        image_rgb = cv2.resize(image_rgb, (800, 800))\n",
    "\n",
    "\n",
    "                        ## **** Mediapipe Holistic Detection ****\n",
    "                        holisticResults = holistic.process(image_rgb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        ## **** Mediapipe hand detection ****\n",
    "#                             image_rgb.flags.writeable = False\n",
    "#                             results = hands.process(image_rgb)\n",
    "#                             image_rgb.flags.writeable = True\n",
    "\n",
    "                        ## convert image back to bgr for outputting it in rgb with the cam (python is confusing...)\n",
    "                        image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                        # Draw the hand annotations on the image.\n",
    "                        #if results.multi_hand_landmarks:\n",
    "                        #    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        #        mp_drawing.draw_landmarks(\n",
    "                        #            image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        #gesture recongition output and pose recognition output\n",
    "                        debug_image = copy.deepcopy(image_rgb)\n",
    "                        holistic_image = copy.deepcopy(image_rgb)\n",
    "\n",
    "\n",
    "                        ## Draw the holistic annotations on the image\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            holistic_image,\n",
    "                            holisticResults.face_landmarks,\n",
    "                            mp_holistic.FACEMESH_CONTOURS,\n",
    "                            landmark_drawing_spec=None,\n",
    "                            connection_drawing_spec=mp_drawing_styles\n",
    "                            .get_default_face_mesh_contours_style())\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            holistic_image,\n",
    "                            holisticResults.pose_landmarks,\n",
    "                            mp_holistic.POSE_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp_drawing_styles\n",
    "                            .get_default_pose_landmarks_style())\n",
    "                        \n",
    "                        # draw left hand\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            holistic_image,\n",
    "                            holisticResults.left_hand_landmarks,\n",
    "                            mp_holistic.HAND_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp_drawing_styles\n",
    "                            .get_default_hand_landmarks_style())\n",
    "                        # draw right hand\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            holistic_image,\n",
    "                            holisticResults.right_hand_landmarks,\n",
    "                            mp_holistic.HAND_CONNECTIONS,\n",
    "                            landmark_drawing_spec=mp_drawing_styles\n",
    "                            .get_default_hand_landmarks_style())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                             # Draw the hand annotations on the image.\n",
    "#                             if results.multi_hand_landmarks:\n",
    "#                                 for hand_landmarks, handedness in zip(results.multi_hand_landmarks,results.multi_handedness):\n",
    "#                                     mp_drawing.draw_landmarks(image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "#                                     ## gesture recognition\n",
    "\n",
    "#                                     # Bounding box calculation\n",
    "#                                     brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
    "#                                     # Landmark calculation\n",
    "#                                     landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "#                                     ## Landmark calcualtion with 3 coordinates\n",
    "#                                     hand_landmarks_list = get_hand_landmarks_list(debug_image, hand_landmarks)\n",
    "\n",
    "#                                     # Conversion to relative coordinates / normalized coordinates\n",
    "#                                     pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "#                                     pre_processed_point_history_list = pre_process_point_history(debug_image, point_history)\n",
    "#                                     # Write to the dataset file\n",
    "#                                     logging_csv(number, mode, pre_processed_landmark_list, pre_processed_point_history_list)\n",
    "\n",
    "#                                     # Hand sign classification\n",
    "#                                     hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "#                                     if hand_sign_id == 2:  # Point gesture\n",
    "#                                         point_history.append(landmark_list[8])\n",
    "#                                     else:\n",
    "#                                         point_history.append([0, 0])\n",
    "\n",
    "#                                     # Finger gesture classification\n",
    "#                                     finger_gesture_id = 0\n",
    "#                                     point_history_len = len(pre_processed_point_history_list)\n",
    "#                                     if point_history_len == (history_length * 2):\n",
    "#                                         finger_gesture_id = point_history_classifier(pre_processed_point_history_list)\n",
    "\n",
    "#                                     # Calculates the gesture IDs in the latest detection\n",
    "#                                     finger_gesture_history.append(finger_gesture_id)\n",
    "#                                     most_common_fg_id = Counter(finger_gesture_history).most_common()\n",
    "\n",
    "#                                     # Drawing part\n",
    "#                                     debug_image = draw_bounding_rect(use_brect, debug_image, brect)\n",
    "#                                     debug_image = draw_landmarks(debug_image, landmark_list)\n",
    "#                                     debug_image = draw_info_text(\n",
    "#                                         debug_image,\n",
    "#                                         brect,\n",
    "#                                         handedness,\n",
    "#                                         keypoint_classifier_labels[hand_sign_id],\n",
    "#                                         point_history_classifier_labels[most_common_fg_id[0][0]],\n",
    "#                                     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        cv2.imshow('MediaPipe Holistic', holistic_image)         \n",
    "                        #cv2.imshow('Hand Gesture Recognition', debug_image)    \n",
    "                        cv2.imshow('MediaPipe Hands', image_rgb)\n",
    "                        if cv2.waitKey(5) & 0xFF == 27:\n",
    "                            break\n",
    "                                \n",
    "                    #image_result.Release()\n",
    "\n",
    "                    # Getting the image data as a numpy array\n",
    "                    #image_data = image_result.GetNDArray()\n",
    "\n",
    "                    # Draws an image on the current figure\n",
    "                    #plt.imshow(image_data, cmap='gray')\n",
    "\n",
    "                    # Interval in plt.pause(interval) determines how fast the images are displayed in a GUI\n",
    "                    # Interval is in seconds.\n",
    "                    #plt.pause(0.001)\n",
    "\n",
    "                    # Clear current reference of a figure. This will improve display speed significantly\n",
    "                    #plt.clf()\n",
    "\n",
    "                    # If user presses enter, close the program\n",
    "                    if keyboard.is_pressed('ENTER'):\n",
    "                        print('Program is closing...')\n",
    "\n",
    "                        # Close figure\n",
    "                        plt.close('all')             \n",
    "                        input('Done! Press Enter to exit...')\n",
    "                        continue_recording=False                        \n",
    "\n",
    "                    #  Release image\n",
    "                    #\n",
    "                    #  *** NOTES ***\n",
    "                    #  Images retrieved directly from the camera (i.e. non-converted\n",
    "                    #  images) need to be released in order to keep from filling the\n",
    "                    #  buffer.\n",
    "                    image_result.Release()\n",
    "\n",
    "                except PySpin.SpinnakerException as ex:\n",
    "                    print('Error: %s' % ex)\n",
    "                    return False\n",
    "\n",
    "        #  End acquisition\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  Ending acquisition appropriately helps ensure that devices clean up\n",
    "        #  properly and do not need to be power-cycled to maintain integrity.\n",
    "        cam.EndAcquisition()\n",
    "\n",
    "    except PySpin.SpinnakerException as ex:\n",
    "        print('Error: %s' % ex)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gesture recognition\n",
    "\n",
    "## converting hand_landmarks to sendable vector3 string list?\n",
    "## returns a list of all handmarks and their 3 coordinates (normalized)\n",
    "def get_hand_landmarks_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = landmark.x\n",
    "        landmark_y = landmark.y\n",
    "        landmark_z = landmark.z\n",
    "        #landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        #landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        #landmark_z = landmark.z\n",
    "\n",
    "        \n",
    "        ## need the brackets for this function\n",
    "        landmark_point.append([landmark_x, landmark_y, landmark_z])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gesture recognition functions\n",
    "\n",
    "def select_mode(key, mode):\n",
    "    number = -1\n",
    "    if 48 <= key <= 57:  # 0 ~ 9\n",
    "        number = key - 48\n",
    "    if key == 110:  # n\n",
    "        mode = 0\n",
    "    if key == 107:  # k\n",
    "        mode = 1\n",
    "    if key == 104:  # h\n",
    "        mode = 2\n",
    "    return number, mode\n",
    "\n",
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "\n",
    "def pre_process_point_history(image, point_history):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    temp_point_history = copy.deepcopy(point_history)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, point in enumerate(temp_point_history):\n",
    "        if index == 0:\n",
    "            base_x, base_y = point[0], point[1]\n",
    "\n",
    "        temp_point_history[index][0] = (temp_point_history[index][0] -\n",
    "                                        base_x) / image_width\n",
    "        temp_point_history[index][1] = (temp_point_history[index][1] -\n",
    "                                        base_y) / image_height\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_point_history = list(\n",
    "        itertools.chain.from_iterable(temp_point_history))\n",
    "\n",
    "    return temp_point_history\n",
    "\n",
    "\n",
    "def logging_csv(number, mode, landmark_list, point_history_list):\n",
    "    if mode == 0:\n",
    "        pass\n",
    "    if mode == 1 and (0 <= number <= 9):\n",
    "        csv_path = 'model/keypoint_classifier/keypoint.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *landmark_list])\n",
    "    if mode == 2 and (0 <= number <= 9):\n",
    "        csv_path = 'model/point_history_classifier/point_history.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *point_history_list])\n",
    "    return\n",
    "\n",
    "def draw_landmarks(image, landmark_point):\n",
    "    if len(landmark_point) > 0:\n",
    "        # Thumb\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Index finger\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Middle finger\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Ring finger\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Little finger\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Palm\n",
    "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "    # Key Points\n",
    "    for index, landmark in enumerate(landmark_point):\n",
    "        if index == 0:  # 手首1\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 1:  # 手首2\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 2:  # 親指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 3:  # 親指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 4:  # 親指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 5:  # 人差指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 6:  # 人差指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 7:  # 人差指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 8:  # 人差指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 9:  # 中指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 10:  # 中指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 11:  # 中指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 12:  # 中指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 13:  # 薬指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 14:  # 薬指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 15:  # 薬指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 16:  # 薬指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 17:  # 小指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 18:  # 小指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 19:  # 小指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 20:  # 小指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        # Outer rectangle\n",
    "        cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info_text(image, brect, handedness, hand_sign_text,\n",
    "                   finger_gesture_text):\n",
    "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                 (0, 0, 0), -1)\n",
    "\n",
    "    info_text = handedness.classification[0].label[0:]\n",
    "    if hand_sign_text != \"\":\n",
    "        info_text = info_text + ':' + hand_sign_text\n",
    "    cv2.putText(image, info_text, (brect[0] + 5, brect[1] - 4),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    if finger_gesture_text != \"\":\n",
    "        cv2.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
    "                   cv2.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_point_history(image, point_history):\n",
    "    for index, point in enumerate(point_history):\n",
    "        if point[0] != 0 and point[1] != 0:\n",
    "            cv2.circle(image, (point[0], point[1]), 1 + int(index / 2),\n",
    "                      (152, 251, 152), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info(image, fps, mode, number):\n",
    "    cv2.putText(image, \"FPS:\" + str(fps), (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "    cv2.putText(image, \"FPS:\" + str(fps), (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    mode_string = ['Logging Key Point', 'Logging Point History']\n",
    "    if 1 <= mode <= 2:\n",
    "        cv2.putText(image, \"MODE:\" + mode_string[mode - 1], (10, 90),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                   cv2.LINE_AA)\n",
    "        if 0 <= number <= 9:\n",
    "            cv2.putText(image, \"NUM:\" + str(number), (10, 110),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                       cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_single_camera(cam):\n",
    "    \"\"\"\n",
    "    This function acts as the body of the example; please see NodeMapInfo example\n",
    "    for more in-depth comments on setting up cameras.\n",
    "\n",
    "    :param cam: Camera to run on.\n",
    "    :type cam: CameraPtr\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = True\n",
    "\n",
    "        nodemap_tldevice = cam.GetTLDeviceNodeMap()\n",
    "\n",
    "        # Initialize camera\n",
    "        cam.Init()\n",
    "\n",
    "        # Retrieve GenICam nodemap\n",
    "        nodemap = cam.GetNodeMap()\n",
    "\n",
    "        # Acquire images\n",
    "        result &= acquire_and_display_images(cam, nodemap, nodemap_tldevice)\n",
    "\n",
    "        # Deinitialize camera\n",
    "        cam.DeInit()\n",
    "\n",
    "    except PySpin.SpinnakerException as ex:\n",
    "        print('Error: %s' % ex)\n",
    "        result = False\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example entry point; notice the volume of data that the logging event handler\n",
    "    prints out on debug despite the fact that very little really happens in this\n",
    "    example. Because of this, it may be better to have the logger set to lower\n",
    "    level in order to provide a more concise, focused log.\n",
    "\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    result = True\n",
    "\n",
    "    # Retrieve singleton reference to system object\n",
    "    system = PySpin.System.GetInstance()\n",
    "\n",
    "    # Get current library version\n",
    "    version = system.GetLibraryVersion()\n",
    "    print('Library version: %d.%d.%d.%d' % (version.major, version.minor, version.type, version.build))\n",
    "\n",
    "    # Retrieve list of cameras from the system\n",
    "    cam_list = system.GetCameras()\n",
    "\n",
    "    num_cameras = cam_list.GetSize()\n",
    "\n",
    "    print('Number of cameras detected: %d' % num_cameras)\n",
    "\n",
    "    # Finish if there are no cameras\n",
    "    if num_cameras == 0:\n",
    "\n",
    "        # Clear camera list before releasing system\n",
    "        cam_list.Clear()\n",
    "\n",
    "        # Release system instance\n",
    "        system.ReleaseInstance()\n",
    "\n",
    "        print('Not enough cameras!')\n",
    "        input('Done! Press Enter to exit...')\n",
    "        return False\n",
    "\n",
    "    # Run example on each camera\n",
    "    for i, cam in enumerate(cam_list):\n",
    "\n",
    "        print('Running example for camera %d...' % i)\n",
    "\n",
    "        result &= run_single_camera(cam)\n",
    "        print('Camera %d example complete... \\n' % i)\n",
    "\n",
    "    # Release reference to camera\n",
    "    # NOTE: Unlike the C++ examples, we cannot rely on pointer objects being automatically\n",
    "    # cleaned up when going out of scope.\n",
    "    # The usage of del is preferred to assigning the variable to None.\n",
    "    del cam\n",
    "\n",
    "    # Clear camera list before releasing system\n",
    "    cam_list.Clear()\n",
    "\n",
    "    # Release system instance\n",
    "    system.ReleaseInstance()\n",
    "\n",
    "    input('Done! Press Enter to exit...')\n",
    "    cv2.destroyAllWindows()\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if main():\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
