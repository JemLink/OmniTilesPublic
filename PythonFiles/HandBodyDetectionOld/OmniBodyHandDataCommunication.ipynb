{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## another cam aquisition\n",
    "## so far it is way faster :D\n",
    "\n",
    "\n",
    "# coding=utf-8\n",
    "# =============================================================================\n",
    "# Copyright (c) 2001-2021 FLIR Systems, Inc. All Rights Reserved.\n",
    "#\n",
    "# This software is the confidential and proprietary information of FLIR\n",
    "# Integrated Imaging Solutions, Inc. (\"Confidential Information\"). You\n",
    "# shall not disclose such Confidential Information and shall use it only in\n",
    "# accordance with the terms of the license agreement you entered into\n",
    "# with FLIR Integrated Imaging Solutions, Inc. (FLIR).\n",
    "#\n",
    "# FLIR MAKES NO REPRESENTATIONS OR WARRANTIES ABOUT THE SUITABILITY OF THE\n",
    "# SOFTWARE, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR\n",
    "# PURPOSE, OR NON-INFRINGEMENT. FLIR SHALL NOT BE LIABLE FOR ANY DAMAGES\n",
    "# SUFFERED BY LICENSEE AS A RESULT OF USING, MODIFYING OR DISTRIBUTING\n",
    "# THIS SOFTWARE OR ITS DERIVATIVES.\n",
    "# =============================================================================\n",
    "#\n",
    "# This AcquireAndDisplay.py shows how to get the image data, and then display images in a GUI.\n",
    "# This example relies on information provided in the ImageChannelStatistics.py example.\n",
    "#\n",
    "# This example demonstrates how to display images represented as numpy arrays.\n",
    "# Currently, this program is limited to single camera use.\n",
    "# NOTE: keyboard and matplotlib must be installed on Python interpreter prior to running this example.\n",
    "\n",
    "import os\n",
    "from pyspin import PySpin\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import keyboard\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Mediapipe hand tracking and OpenCV and Pillow\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "\n",
    "## gesture recognition\n",
    "from utils import CvFpsCalc\n",
    "from model import KeyPointClassifier\n",
    "from model import PointHistoryClassifier\n",
    "\n",
    "import csv\n",
    "import copy\n",
    "import argparse\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# Unity python communication\n",
    "#import time #already imported above\n",
    "import zmq\n",
    "import random\n",
    "\n",
    "context = zmq.Context()\n",
    "socket = context.socket(zmq.REP)\n",
    "socket.bind(\"tcp://*:5555\")\n",
    "unity_output = \"-1\"\n",
    "\n",
    "\n",
    "# insert the output of fisheye calibration step\n",
    "Img_DIM=(800, 800)\n",
    "dim1=(800, 800)\n",
    "dim2=(800, 800)\n",
    "dim3=(800, 800)\n",
    "balance = 1.0\n",
    "K=np.array([[351.6318033232932, 0.0, 450.4607527123814], [0.0, 351.7439651791754, 391.2576486450391], [0.0, 0.0, 1.0]])\n",
    "D=np.array([[-0.170592708935433], [0.5324235902617314], [-1.5452235955907878], [1.4793950832426657]])\n",
    "\n",
    "\n",
    "ROI_y = 850\n",
    "ROI_x = 550\n",
    "ROI_height = 800\n",
    "ROI_width = 800\n",
    "\n",
    "\n",
    "\n",
    "global continue_recording\n",
    "continue_recording = True\n",
    "\n",
    "\n",
    "BG_COLOR = (0, 0, 0) # black\n",
    "\n",
    "\n",
    "def handle_close(evt):\n",
    "    \"\"\"\n",
    "    This function will close the GUI when close event happens.\n",
    "\n",
    "    :param evt: Event that occurs when the figure closes.\n",
    "    :type evt: Event\n",
    "    \"\"\"\n",
    "\n",
    "    global continue_recording\n",
    "    continue_recording = False\n",
    "\n",
    "\n",
    "def acquire_and_display_images(cam, nodemap, nodemap_tldevice):\n",
    "    \"\"\"\n",
    "    This function continuously acquires images from a device and display them in a GUI.\n",
    "\n",
    "    :param cam: Camera to acquire images from.\n",
    "    :param nodemap: Device nodemap.\n",
    "    :param nodemap_tldevice: Transport layer device nodemap.\n",
    "    :type cam: CameraPtr\n",
    "    :type nodemap: INodeMap\n",
    "    :type nodemap_tldevice: INodeMap\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    global continue_recording\n",
    "\n",
    "    sNodemap = cam.GetTLStreamNodeMap()\n",
    "\n",
    "    # Change bufferhandling mode to NewestOnly\n",
    "    node_bufferhandling_mode = PySpin.CEnumerationPtr(sNodemap.GetNode('StreamBufferHandlingMode'))\n",
    "    if not PySpin.IsAvailable(node_bufferhandling_mode) or not PySpin.IsWritable(node_bufferhandling_mode):\n",
    "        print('Unable to set stream buffer handling mode.. Aborting...')\n",
    "        return False\n",
    "\n",
    "    # Retrieve entry node from enumeration node\n",
    "    node_newestonly = node_bufferhandling_mode.GetEntryByName('NewestOnly')\n",
    "    if not PySpin.IsAvailable(node_newestonly) or not PySpin.IsReadable(node_newestonly):\n",
    "        print('Unable to set stream buffer handling mode.. Aborting...')\n",
    "        return False\n",
    "\n",
    "    # Retrieve integer value from entry node\n",
    "    node_newestonly_mode = node_newestonly.GetValue()\n",
    "\n",
    "    # Set integer value from entry node as new value of enumeration node\n",
    "    node_bufferhandling_mode.SetIntValue(node_newestonly_mode)\n",
    "\n",
    "    print('*** IMAGE ACQUISITION ***\\n')\n",
    "    try:\n",
    "        ## gesture recognition\n",
    "        result = True\n",
    "        use_brect = True\n",
    "        \n",
    "        ## media pipe\n",
    "        node_acquisition_mode = PySpin.CEnumerationPtr(nodemap.GetNode('AcquisitionMode'))\n",
    "        if not PySpin.IsAvailable(node_acquisition_mode) or not PySpin.IsWritable(node_acquisition_mode):\n",
    "            print('Unable to set acquisition mode to continuous (enum retrieval). Aborting...')\n",
    "            return False\n",
    "\n",
    "        # Retrieve entry node from enumeration node\n",
    "        node_acquisition_mode_continuous = node_acquisition_mode.GetEntryByName('Continuous')\n",
    "        if not PySpin.IsAvailable(node_acquisition_mode_continuous) or not PySpin.IsReadable(\n",
    "                node_acquisition_mode_continuous):\n",
    "            print('Unable to set acquisition mode to continuous (entry retrieval). Aborting...')\n",
    "            return False\n",
    "\n",
    "        # Retrieve integer value from entry node\n",
    "        acquisition_mode_continuous = node_acquisition_mode_continuous.GetValue()\n",
    "\n",
    "        # Set integer value from entry node as new value of enumeration node\n",
    "        node_acquisition_mode.SetIntValue(acquisition_mode_continuous)\n",
    "\n",
    "        print('Acquisition mode set to continuous...')\n",
    "        \n",
    "        \n",
    "        ## fisheye parameter\n",
    "        scaled_K = K * dim1[0] / Img_DIM[0]  # The values of K is to scale with image dimension.\n",
    "        scaled_K[2][2] = 1.0  # Except that K[2][2] is always 1.0\n",
    "    \n",
    "        # This is how scaled_K, dim2 and balance are used to determine the final K used to un-distort image. OpenCV document failed to make this clear!\n",
    "        new_K = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(scaled_K, D, dim2, np.eye(3), balance=balance)\n",
    "        map1, map2 = cv2.fisheye.initUndistortRectifyMap(scaled_K, D, np.eye(3), new_K, dim3, cv2.CV_16SC2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        #  Begin acquiring images\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  What happens when the camera begins acquiring images depends on the\n",
    "        #  acquisition mode. Single frame captures only a single image, multi\n",
    "        #  frame catures a set number of images, and continuous captures a\n",
    "        #  continuous stream of images.\n",
    "        #\n",
    "        #  *** LATER ***\n",
    "        #  Image acquisition must be ended when no more images are needed.\n",
    "        cam.BeginAcquisition()\n",
    "\n",
    "        print('Acquiring images...')\n",
    "\n",
    "        #  Retrieve device serial number for filename\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  The device serial number is retrieved in order to keep cameras from\n",
    "        #  overwriting one another. Grabbing image IDs could also accomplish\n",
    "        #  this.\n",
    "        device_serial_number = ''\n",
    "        node_device_serial_number = PySpin.CStringPtr(nodemap_tldevice.GetNode('DeviceSerialNumber'))\n",
    "        if PySpin.IsAvailable(node_device_serial_number) and PySpin.IsReadable(node_device_serial_number):\n",
    "            device_serial_number = node_device_serial_number.GetValue()\n",
    "            print('Device serial number retrieved as %s...' % device_serial_number)\n",
    "\n",
    "        # Close program\n",
    "        print('Press enter to close the program..')\n",
    "\n",
    "        # Figure(1) is default so you can omit this line. Figure(0) will create a new window every time program hits this line\n",
    "        fig = plt.figure(1)\n",
    "\n",
    "        # Close the GUI when close event happens\n",
    "        fig.canvas.mpl_connect('close_event', handle_close)\n",
    "        \n",
    "        print('Waiting for Unity3D to start communication')\n",
    "        \n",
    "        ## this is for the pose aquisition\n",
    "        with mp_pose.Pose(\n",
    "            enable_segmentation=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as pose:\n",
    "        \n",
    "\n",
    "            # Retrieve and display images\n",
    "            with mp_hands.Hands(\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "                ## for gesture recognition\n",
    "                keypoint_classifier = KeyPointClassifier()\n",
    "\n",
    "                point_history_classifier = PointHistoryClassifier()\n",
    "\n",
    "                # Read labels ###########################################################\n",
    "                with open('model/keypoint_classifier/keypoint_classifier_label.csv', encoding='utf-8-sig') as f:\n",
    "                    keypoint_classifier_labels = csv.reader(f)\n",
    "                    keypoint_classifier_labels = [row[0] for row in keypoint_classifier_labels]\n",
    "                with open('model/point_history_classifier/point_history_classifier_label.csv', encoding='utf-8-sig') as f:\n",
    "                    point_history_classifier_labels = csv.reader(f)\n",
    "                    point_history_classifier_labels = [row[0] for row in point_history_classifier_labels]\n",
    "\n",
    "                # FPS Measurement ########################################################\n",
    "                cvFpsCalc = CvFpsCalc(buffer_len=10)\n",
    "\n",
    "                # Coordinate history #################################################################\n",
    "                history_length = 16\n",
    "                point_history = deque(maxlen=history_length)\n",
    "\n",
    "                # Finger gesture history ################################################\n",
    "                finger_gesture_history = deque(maxlen=history_length)\n",
    "\n",
    "                #  ########################################################################\n",
    "\n",
    "                mode = 0\n",
    "\n",
    "                while(continue_recording):\n",
    "                    try:\n",
    "\n",
    "\n",
    "                        fps = cvFpsCalc.get()\n",
    "                        # Process Key (ESC: end) #################################################\n",
    "                        key = cv2.waitKey(1)\n",
    "                        if key == 27:  # ESC\n",
    "                            break\n",
    "                        number, mode = select_mode(key, mode)\n",
    "\n",
    "\n",
    "                        \n",
    "                            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        #  Retrieve next received image\n",
    "                        #\n",
    "                        #  *** NOTES ***\n",
    "                        #  Capturing an image houses images on the camera buffer. Trying\n",
    "                        #  to capture an image that does not exist will hang the camera.\n",
    "                        #\n",
    "                        #  *** LATER ***\n",
    "                        #  Once an image from the buffer is saved and/or no longer\n",
    "                        #  needed, the image must be released in order to keep the\n",
    "                        #  buffer from filling up.\n",
    "\n",
    "                        image_result = cam.GetNextImage()\n",
    "\n",
    "                        #  Ensure image completion\n",
    "                        if image_result.IsIncomplete():\n",
    "                            print('Image incomplete with image status %d ...' % image_result.GetImageStatus())\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            ## This is 2048x2048 so we need to crop it to the ROI from the lense\n",
    "                            width = image_result.GetWidth()\n",
    "                            height = image_result.GetHeight()\n",
    "\n",
    "\n",
    "                            ## This converts it to GreyScale\n",
    "                            #image_converted = image_result.Convert(spin.PixelFormat_Mono8, spin.HQ_LINEAR)\n",
    "                            ## This converts it to RGB\n",
    "                            image_converted = image_result.Convert(PySpin.PixelFormat_BGR8)\n",
    "                            rgb_array = image_converted.GetData()\n",
    "                            rgb_array = rgb_array.reshape(height, width, 3)\n",
    "\n",
    "\n",
    "                            ## process mediapipe on image\n",
    "                            #image_rgb = cv2.cvtColor(cv2.flip(rgb_array, 1), cv2.COLOR_BGR2RGB)\n",
    "                            image_rgb = rgb_array#cv2.flip(rgb_array, 1)\n",
    "                            \n",
    "                            \n",
    "                            ## unity communication\n",
    "                            message = socket.recv()\n",
    "                            print(\"Received request: %s\" % message)\n",
    "                            unity_output = \"-1\" # when no gesture/pose is detected -1 will be returned\n",
    "\n",
    "                            stringMessage = message.decode(\"utf-8\")\n",
    "                            stringMessage = stringMessage.strip()\n",
    "                            ## this is to stop the running program\n",
    "                            if stringMessage == \"END\":\n",
    "                                print(\"Should END\")\n",
    "                                ## create the data that contains a string for the gesture etc and an iamge of the pose\n",
    "                                data = {\n",
    "                                    'messageString': 'END',\n",
    "                                    'image': cv2.imencode('.jpg', image_rgb)[1].ravel().tolist()\n",
    "                                }\n",
    "                                #output_byte = str.encode(\"END\") #this was just for gesture\n",
    "                                socket.send_json(data)\n",
    "                                break\n",
    "\n",
    "\n",
    "                            ## **** Resizing / Croping *****\n",
    "\n",
    "                            ## RESIZING the image since it would be 2048x2048 otherwise (kind of too big for the window)\n",
    "\n",
    "                            ## we might have to size this further down for mediapipe to run fast\n",
    "                            image_rgb = cv2.resize(image_rgb, (800, 800))\n",
    "                            # scale = 800/2048   \n",
    "                            ## this is to display potential cropping region in the downscaled image\n",
    "                            # cv2.rectangle(image_rgb, (int(650*scale), int(580*scale)), (int(1450*scale), int(1380*scale)), (0, 255, 0), 3)\n",
    "\n",
    "                            ## CROPPPING the region of the lense (should be around 800 to fit with the setup so far...)\n",
    "                            ## This is the cropping\n",
    "                            ## These values are taken from the unity config\n",
    "                            ## They might change...\n",
    "                            #array_cropped = image_rgb[ROI_y:(ROI_y + ROI_height), ROI_x:(ROI_x + ROI_width)]\n",
    "                            #image_rgb = array_cropped.copy() # needed to get the correct data format for further processing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            ## **** un-distort image\n",
    "\n",
    "                            #undistorted_img = cv2.remap(image_rgb, map1, map2, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "                            image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_BGR2RGB)  \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            ## **** Mediapipe Pose Detection ****\n",
    "                            image_rgb.flags.writeable = False\n",
    "                            poseResults = pose.process(image_rgb)\n",
    "                            ## **** Mediapipe hand detection ****\n",
    "                            results = hands.process(image_rgb)\n",
    "                            image_rgb.flags.writeable = True\n",
    "\n",
    "                            \n",
    "                            ## convert image back to bgr for outputting it in rgb with the cam (python is confusing...)\n",
    "                            image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "\n",
    "                            #gesture recongition output and pose recognition output\n",
    "                            debug_image = copy.deepcopy(image_rgb)\n",
    "                            pose_image = copy.deepcopy(image_rgb)\n",
    "                            \n",
    "                            \n",
    "                            output_image = np.zeros(pose_image.shape, dtype=np.uint8)\n",
    "                            output_image[:] = BG_COLOR\n",
    "                            if poseResults.pose_landmarks:\n",
    "                                ## Draw the pose annotations on the image\n",
    "#                                 mp_drawing.draw_landmarks(\n",
    "#                                     pose_image,\n",
    "#                                     poseResults.pose_landmarks,\n",
    "#                                     mp_pose.POSE_CONNECTIONS,\n",
    "#                                     landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()) \n",
    "\n",
    "                                #annotated_image = pose_image.copy()\n",
    "                                # Draw segmentation on the image.\n",
    "                                # To improve segmentation around boundaries, consider applying a joint\n",
    "                                # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
    "                                 condition = np.stack((poseResults.segmentation_mask,) * 3, axis=-1) > 0.1\n",
    "                                 bg_image = np.zeros(pose_image.shape, dtype=np.uint8)\n",
    "                                 bg_image[:] = BG_COLOR\n",
    "                                 output_image = np.where(condition, pose_image, bg_image)\n",
    "                            \n",
    "\n",
    "                            # Draw the hand annotations on the image.\n",
    "                            if results.multi_hand_landmarks:\n",
    "\n",
    "                                # need to overwrite unity_output so that it will not start with -1\n",
    "                                unity_output = \"\"\n",
    "\n",
    "                                for hand_landmarks, handedness in zip(results.multi_hand_landmarks,results.multi_handedness):\n",
    "                                    mp_drawing.draw_landmarks(image_rgb, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                                    ## gesture recognition\n",
    "\n",
    "                                    # Bounding box calculation\n",
    "                                    brect = calc_bounding_rect(debug_image, hand_landmarks)\n",
    "                                    # Landmark calculation\n",
    "                                    landmark_list = calc_landmark_list(debug_image, hand_landmarks)\n",
    "\n",
    "                                    ## Landmark calcualtion with 3 coordinates\n",
    "                                    hand_landmarks_list = get_hand_landmarks_list(debug_image, hand_landmarks)\n",
    "\n",
    "                                    # Conversion to relative coordinates / normalized coordinates\n",
    "                                    pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "                                    pre_processed_point_history_list = pre_process_point_history(debug_image, point_history)\n",
    "                                    # Write to the dataset file\n",
    "                                    logging_csv(number, mode, pre_processed_landmark_list, pre_processed_point_history_list)\n",
    "\n",
    "                                    # Hand sign classification\n",
    "                                    hand_sign_id = keypoint_classifier(pre_processed_landmark_list)\n",
    "                                    if hand_sign_id == 2:  # Point gesture\n",
    "                                        point_history.append(landmark_list[8])\n",
    "                                    else:\n",
    "                                        point_history.append([0, 0])\n",
    "\n",
    "                                    # Finger gesture classification\n",
    "                                    finger_gesture_id = 0\n",
    "                                    point_history_len = len(pre_processed_point_history_list)\n",
    "                                    if point_history_len == (history_length * 2):\n",
    "                                        finger_gesture_id = point_history_classifier(pre_processed_point_history_list)\n",
    "\n",
    "                                    # Calculates the gesture IDs in the latest detection\n",
    "                                    finger_gesture_history.append(finger_gesture_id)\n",
    "                                    most_common_fg_id = Counter(finger_gesture_history).most_common()\n",
    "\n",
    "                                    # Drawing part\n",
    "                                    debug_image = draw_bounding_rect(use_brect, debug_image, brect)\n",
    "                                    debug_image = draw_landmarks(debug_image, landmark_list)\n",
    "                                    debug_image = draw_info_text(\n",
    "                                        debug_image,\n",
    "                                        brect,\n",
    "                                        handedness,\n",
    "                                        keypoint_classifier_labels[hand_sign_id],\n",
    "                                        point_history_classifier_labels[most_common_fg_id[0][0]],\n",
    "                                    )\n",
    "\n",
    "                                    # writing hand_sign to unity output\n",
    "                                    unity_output = unity_output + str(hand_sign_id)\n",
    "\n",
    "                                    ## Landmark calcualtion with 3 coordinates\n",
    "                                    hand_landmarks_list = get_hand_landmarks_list(debug_image, hand_landmarks)\n",
    "                                    ## convert hand_landmarks_list to string\n",
    "                                    string_list = \";\".join(str(x) for x in hand_landmarks_list)\n",
    "                                    #print(\"normalized hand_landmarks: \" + string_list)\n",
    "\n",
    "                                    # add an \"h\" for showing that one hand is finished\n",
    "                                    unity_output = unity_output + \":\" + string_list + \"h\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            cv2.imshow('Hand Gesture Recognition', debug_image)    \n",
    "                            cv2.imshow('MediaPipe Hands', image_rgb)\n",
    "                            cv2.imshow('MediaPipe Pose Segmentation mask', output_image) \n",
    "                            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                                break\n",
    "\n",
    "\n",
    "                            ## write gesture output to send to unity\n",
    "                            #output_byte = str.encode(unity_output)\n",
    "                            data = {\n",
    "                                'messageString': unity_output,\n",
    "                                'image': cv2.imencode('.jpg', output_image)[1].ravel().tolist()\n",
    "                            }\n",
    "                            \n",
    "\n",
    "                            #  Send reply back to client\n",
    "                            #  In the real world usage, after you finish your work, send your output here\n",
    "                            socket.send_json(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        #image_result.Release()\n",
    "\n",
    "                        # Getting the image data as a numpy array\n",
    "                        #image_data = image_result.GetNDArray()\n",
    "\n",
    "                        # Draws an image on the current figure\n",
    "                        #plt.imshow(image_data, cmap='gray')\n",
    "\n",
    "                        # Interval in plt.pause(interval) determines how fast the images are displayed in a GUI\n",
    "                        # Interval is in seconds.\n",
    "                        #plt.pause(0.001)\n",
    "\n",
    "                        # Clear current reference of a figure. This will improve display speed significantly\n",
    "                        #plt.clf()\n",
    "\n",
    "                        # If user presses enter, close the program\n",
    "                        if keyboard.is_pressed('ENTER'):\n",
    "                            print('Program is closing...')\n",
    "\n",
    "                            # Close figure\n",
    "                            plt.close('all')             \n",
    "                            input('Done! Press Enter to exit...')\n",
    "                            continue_recording=False                        \n",
    "\n",
    "                        #  Release image\n",
    "                        #\n",
    "                        #  *** NOTES ***\n",
    "                        #  Images retrieved directly from the camera (i.e. non-converted\n",
    "                        #  images) need to be released in order to keep from filling the\n",
    "                        #  buffer.\n",
    "                        image_result.Release()\n",
    "\n",
    "                    except PySpin.SpinnakerException as ex:\n",
    "                        print('Error: %s' % ex)\n",
    "                        return False\n",
    "\n",
    "        #  End acquisition\n",
    "        #\n",
    "        #  *** NOTES ***\n",
    "        #  Ending acquisition appropriately helps ensure that devices clean up\n",
    "        #  properly and do not need to be power-cycled to maintain integrity.\n",
    "        cam.EndAcquisition()\n",
    "\n",
    "    except PySpin.SpinnakerException as ex:\n",
    "        print('Error: %s' % ex)\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gesture recognition\n",
    "\n",
    "## converting hand_landmarks to sendable vector3 string list?\n",
    "## returns a list of all handmarks and their 3 coordinates (normalized)\n",
    "def get_hand_landmarks_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = landmark.x\n",
    "        landmark_y = landmark.y\n",
    "        landmark_z = landmark.z\n",
    "        #landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        #landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        #landmark_z = landmark.z\n",
    "\n",
    "        \n",
    "        ## need the brackets for this function\n",
    "        landmark_point.append([landmark_x, landmark_y, landmark_z])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## gesture recognition functions\n",
    "\n",
    "def select_mode(key, mode):\n",
    "    number = -1\n",
    "    if 48 <= key <= 57:  # 0 ~ 9\n",
    "        number = key - 48\n",
    "    if key == 110:  # n\n",
    "        mode = 0\n",
    "    if key == 107:  # k\n",
    "        mode = 1\n",
    "    if key == 104:  # h\n",
    "        mode = 2\n",
    "    return number, mode\n",
    "\n",
    "def calc_bounding_rect(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0, 2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis=0)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(landmark_array)\n",
    "\n",
    "    return [x, y, x + w, y + h]\n",
    "\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_point = []\n",
    "\n",
    "    # Keypoint\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        # landmark_z = landmark.z\n",
    "\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        if index == 0:\n",
    "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
    "\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_landmark_list = list(\n",
    "        itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "\n",
    "    def normalize_(n):\n",
    "        return n / max_value\n",
    "\n",
    "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
    "\n",
    "    return temp_landmark_list\n",
    "\n",
    "\n",
    "def pre_process_point_history(image, point_history):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    temp_point_history = copy.deepcopy(point_history)\n",
    "\n",
    "    # Convert to relative coordinates\n",
    "    base_x, base_y = 0, 0\n",
    "    for index, point in enumerate(temp_point_history):\n",
    "        if index == 0:\n",
    "            base_x, base_y = point[0], point[1]\n",
    "\n",
    "        temp_point_history[index][0] = (temp_point_history[index][0] -\n",
    "                                        base_x) / image_width\n",
    "        temp_point_history[index][1] = (temp_point_history[index][1] -\n",
    "                                        base_y) / image_height\n",
    "\n",
    "    # Convert to a one-dimensional list\n",
    "    temp_point_history = list(\n",
    "        itertools.chain.from_iterable(temp_point_history))\n",
    "\n",
    "    return temp_point_history\n",
    "\n",
    "\n",
    "def logging_csv(number, mode, landmark_list, point_history_list):\n",
    "    if mode == 0:\n",
    "        pass\n",
    "    if mode == 1 and (0 <= number <= 9):\n",
    "        csv_path = 'model/keypoint_classifier/keypoint.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *landmark_list])\n",
    "    if mode == 2 and (0 <= number <= 9):\n",
    "        csv_path = 'model/point_history_classifier/point_history.csv'\n",
    "        with open(csv_path, 'a', newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([number, *point_history_list])\n",
    "    return\n",
    "\n",
    "def draw_landmarks(image, landmark_point):\n",
    "    if len(landmark_point) > 0:\n",
    "        # Thumb\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[3]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[3]), tuple(landmark_point[4]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Index finger\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[6]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[6]), tuple(landmark_point[7]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[7]), tuple(landmark_point[8]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Middle finger\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[10]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[10]), tuple(landmark_point[11]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[11]), tuple(landmark_point[12]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Ring finger\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[14]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[14]), tuple(landmark_point[15]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[15]), tuple(landmark_point[16]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Little finger\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[18]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[18]), tuple(landmark_point[19]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[19]), tuple(landmark_point[20]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "        # Palm\n",
    "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[0]), tuple(landmark_point[1]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[1]), tuple(landmark_point[2]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[2]), tuple(landmark_point[5]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[5]), tuple(landmark_point[9]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[9]), tuple(landmark_point[13]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[13]), tuple(landmark_point[17]),\n",
    "                (255, 255, 255), 2)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (0, 0, 0), 6)\n",
    "        cv2.line(image, tuple(landmark_point[17]), tuple(landmark_point[0]),\n",
    "                (255, 255, 255), 2)\n",
    "\n",
    "    # Key Points\n",
    "    for index, landmark in enumerate(landmark_point):\n",
    "        if index == 0:  # 手首1\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 1:  # 手首2\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 2:  # 親指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 3:  # 親指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 4:  # 親指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 5:  # 人差指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 6:  # 人差指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 7:  # 人差指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 8:  # 人差指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 9:  # 中指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 10:  # 中指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 11:  # 中指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 12:  # 中指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 13:  # 薬指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 14:  # 薬指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 15:  # 薬指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 16:  # 薬指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "        if index == 17:  # 小指：付け根\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 18:  # 小指：第2関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 19:  # 小指：第1関節\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 5, (0, 0, 0), 1)\n",
    "        if index == 20:  # 小指：指先\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (255, 255, 255),\n",
    "                      -1)\n",
    "            cv2.circle(image, (landmark[0], landmark[1]), 8, (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_bounding_rect(use_brect, image, brect):\n",
    "    if use_brect:\n",
    "        # Outer rectangle\n",
    "        cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[3]),\n",
    "                     (0, 0, 0), 1)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info_text(image, brect, handedness, hand_sign_text,\n",
    "                   finger_gesture_text):\n",
    "    cv2.rectangle(image, (brect[0], brect[1]), (brect[2], brect[1] - 22),\n",
    "                 (0, 0, 0), -1)\n",
    "\n",
    "    info_text = handedness.classification[0].label[0:]\n",
    "    if hand_sign_text != \"\":\n",
    "        info_text = info_text + ':' + hand_sign_text\n",
    "    cv2.putText(image, info_text, (brect[0] + 5, brect[1] - 4),\n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "    if finger_gesture_text != \"\":\n",
    "        cv2.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "        cv2.putText(image, \"Finger Gesture:\" + finger_gesture_text, (10, 60),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2,\n",
    "                   cv2.LINE_AA)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_point_history(image, point_history):\n",
    "    for index, point in enumerate(point_history):\n",
    "        if point[0] != 0 and point[1] != 0:\n",
    "            cv2.circle(image, (point[0], point[1]), 1 + int(index / 2),\n",
    "                      (152, 251, 152), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def draw_info(image, fps, mode, number):\n",
    "    cv2.putText(image, \"FPS:\" + str(fps), (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "    cv2.putText(image, \"FPS:\" + str(fps), (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               1.0, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    mode_string = ['Logging Key Point', 'Logging Point History']\n",
    "    if 1 <= mode <= 2:\n",
    "        cv2.putText(image, \"MODE:\" + mode_string[mode - 1], (10, 90),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                   cv2.LINE_AA)\n",
    "        if 0 <= number <= 9:\n",
    "            cv2.putText(image, \"NUM:\" + str(number), (10, 110),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1,\n",
    "                       cv2.LINE_AA)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_single_camera(cam):\n",
    "    \"\"\"\n",
    "    This function acts as the body of the example; please see NodeMapInfo example\n",
    "    for more in-depth comments on setting up cameras.\n",
    "\n",
    "    :param cam: Camera to run on.\n",
    "    :type cam: CameraPtr\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = True\n",
    "\n",
    "        nodemap_tldevice = cam.GetTLDeviceNodeMap()\n",
    "\n",
    "        # Initialize camera\n",
    "        cam.Init()\n",
    "\n",
    "        # Retrieve GenICam nodemap\n",
    "        nodemap = cam.GetNodeMap()\n",
    "\n",
    "        # Acquire images\n",
    "        result &= acquire_and_display_images(cam, nodemap, nodemap_tldevice)\n",
    "\n",
    "        # Deinitialize camera\n",
    "        cam.DeInit()\n",
    "\n",
    "    except PySpin.SpinnakerException as ex:\n",
    "        print('Error: %s' % ex)\n",
    "        result = False\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example entry point; notice the volume of data that the logging event handler\n",
    "    prints out on debug despite the fact that very little really happens in this\n",
    "    example. Because of this, it may be better to have the logger set to lower\n",
    "    level in order to provide a more concise, focused log.\n",
    "\n",
    "    :return: True if successful, False otherwise.\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    result = True\n",
    "\n",
    "    # Retrieve singleton reference to system object\n",
    "    system = PySpin.System.GetInstance()\n",
    "\n",
    "    # Get current library version\n",
    "    version = system.GetLibraryVersion()\n",
    "    print('Library version: %d.%d.%d.%d' % (version.major, version.minor, version.type, version.build))\n",
    "\n",
    "    # Retrieve list of cameras from the system\n",
    "    cam_list = system.GetCameras()\n",
    "\n",
    "    num_cameras = cam_list.GetSize()\n",
    "\n",
    "    print('Number of cameras detected: %d' % num_cameras)\n",
    "\n",
    "    # Finish if there are no cameras\n",
    "    if num_cameras == 0:\n",
    "\n",
    "        # Clear camera list before releasing system\n",
    "        cam_list.Clear()\n",
    "\n",
    "        # Release system instance\n",
    "        system.ReleaseInstance()\n",
    "\n",
    "        print('Not enough cameras!')\n",
    "        input('Done! Press Enter to exit...')\n",
    "        return False\n",
    "\n",
    "    # Run example on each camera\n",
    "    for i, cam in enumerate(cam_list):\n",
    "\n",
    "        print('Running example for camera %d...' % i)\n",
    "\n",
    "        result &= run_single_camera(cam)\n",
    "        print('Camera %d example complete... \\n' % i)\n",
    "\n",
    "    # Release reference to camera\n",
    "    # NOTE: Unlike the C++ examples, we cannot rely on pointer objects being automatically\n",
    "    # cleaned up when going out of scope.\n",
    "    # The usage of del is preferred to assigning the variable to None.\n",
    "    del cam\n",
    "\n",
    "    # Clear camera list before releasing system\n",
    "    cam_list.Clear()\n",
    "\n",
    "    # Release system instance\n",
    "    system.ReleaseInstance()\n",
    "\n",
    "    input('Done! Press Enter to exit...')\n",
    "    cv2.destroyAllWindows()\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if main():\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        sys.exit(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
